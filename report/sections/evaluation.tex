\chapter{Evaluation}

\section{Experiments}

We design our experiments to answer the following research questions:
\begin{enumerate}
    \item Does \name produce better ontologies than traditional methods by subtask composition?
    \item Can \name be easily adapted to a new domain?
\end{enumerate}
We approach the questions by training \name on the Wikipedia dataset and further transfer the model to arXiv with a small number of arXiv samples. As baselines, we use two relation extraction methods, Hearst patterns \cite{hearst1998automated,roller2018hearst} and REBEL \cite{cabot2021rebel}. Relation extraction depends on successful concept discovery to produce high-quality ontologies. To estimate a ceiling to such baselines, \emph{we give the baselines a substantial advantage} by providing them with the ground truth concepts in the test graph. The results show that even with such an advantage, \name outperforms the baselines on many metrics, demonstrating the potential of \name for end-to-end OL.

\subsection{Implementation details}  \label{sec:implementation}


% \subsection{Masked loss regularisation}  \label{sec:method:masked-loss}

\subsection{Results}  \label{sec:results}

\input{figures/metrics}

Our evaluation results reveal that \name produces both semantically and structurally more accurate ontologies than our baselines. Inspecting the metrics for the Wikipedia task in \cref{table:metrics}, we see that although \name is outperformed by the \textbf{Memorisation} and \textbf{Finetune} on Literal F1, it is much better at the Fuzzy, Continuous and Graph F1 metrics. This suggests that while \name produces ontologies that are \emph{syntactically} less aligned to the ground truth, it better captures the overall semantics. In fact, our prompting baselines following the same task format as \name also outperform \textbf{Hearst} and \textbf{REBEL} in the semantics-aware metrics, though they suffer in structural integrity as reflected by the high Motif Distance. The results also hint at the potential pitfalls of syntax-based evaluation metrics as we see syntactic similarity does not generally entail semantic similarity.

The arXiv task differs from the Wikipedia task as it has much fewer relations and there is even less overlap between the train and test split. This imposes a great challenge on \textbf{Finetune} and \name as they need to generalise with a limited diversity of training samples. Despite such constraints, \name is substantially better than other methods in modelling the semantics of the test graph. Inspecting the generated outputs, we observe prompting baselines tend to produce repetitive concepts such as ``Machine Learning and Artificial Intelligence'' and ''Artificial Intelligence and Machine Learning'' while \textbf{Hearst} and \textbf{REBEL} put ``Machine Learning'' as the parent concept of almost all ground truth concepts. Plots for the generated graphs can be found in \cref{appendix:visualisation}.

