\chapter{Evaluation}

\section{Experiments}

We design our experiments to answer the following research questions:
\begin{enumerate}
    \item Does \name produce better ontologies than traditional methods by subtask composition?
    \item Can \name be easily adapted to a new domain?
\end{enumerate}
We approach the questions by training \name on the Wikipedia dataset and further transfer the model to arXiv with a small number of arXiv samples. As baselines, we use two relation extraction methods, Hearst patterns \cite{hearst1998automated,roller2018hearst} and REBEL \cite{cabot2021rebel}. Relation extraction depends on successful concept discovery to produce high-quality ontologies. To estimate a ceiling to such baselines, \emph{we give the baselines a substantial advantage} by providing them with the ground truth concepts in the test graph. The results show that even with such an advantage, \name outperforms the baselines on many metrics, demonstrating the potential of \name for end-to-end OL.

\subsection{Implementation details}  \label{sec:implementation}


% \subsection{Masked loss regularisation}  \label{sec:method:masked-loss}

\input{figures/vanilla_vs_masked}

We discover that directly finetuning an LLM on the sequences defined in \cref{sec:method:subgraph} produces poor results due to overfitting. Analysing the per-token loss of a naively finetuned model on the test split shows that the model tends to memorise high-level relations from the training set, leading to poor generalisation as shown in \cref{fig:vanilla-vs-mask} (top). This occurs because high-level relations are present in many relevant subgraphs and thus repeated many times in the training set. This problem is not solvable by early stopping since terminating training early will result in a model that massively underfits lower-level relations.

This issue is akin to multi-task learning \cite{caruana1997multitask} where the standard solution is to apply some loss weighting factor to rebalance training objectives \cite{sermanet2013overfeat,kendall2018multi}. We draw inspiration from this connection and propose a new training objective that randomly masks the loss contribution from frequently occurring relations. Suppose a relation $u \to v$ is present $n$ times in the training set. During training, when $u \to v$ appears in one of the relevant paths, we mask the tokens for $v$ with probability $\max(1 - M/n, 0)$, where $M$ is a constant for the average number of times a relation is present in the training set. Note that while $v$ is masked from the target, its tokens are still present in the input sequence as context for later tokens. A concrete example is shown in \cref{fig:prompt-template} (right).

We finetune Mistral 7B v0.2 \cite{jiang2023mistral} with Low-Rank Adaptation \cite{hu2021lora} on the masked loss objective. The model is trained on the Wikipedia dataset for two epochs with Adam. During inference, the outputs are generated with temperature 0.1 and nucleus sampling \cite{holtzman2019curious} top-$p$ of 0.9. The weight of each edge is given by the number of generated subgraphs in which it appears. We include a finetuning baseline without the masked loss objective, denoted as \textbf{Finetune}. To adapt \name for arXiv, we further finetune the model on 2048 document-subgraph pairs from arXiv. We initialise new low-rank adaptors and train until the loss stops improving on the validation set. We name these models \textbf{\name (transfer)} and \textbf{Finetune (transfer)} for training without and without the masked loss objective respectively. Full details for the Wikipedia and arXiv experiments can be found in \cref{appendix:training-details}.

The hyperparameters for the post-processing steps are tuned by grid search on the validation set. We sweep over $\alpha \in 1 - \text{geomspace}(1 / |E_\text{raw}|, 1, 21)$ and $\beta \in \text{geomspace}(0.1, 1, 21) - 0.1$ and use the values that maximises the continuous F1 metric. For Wikipedia, we choose the subgraph modelling path length $N=4$ as it is the smallest $N$ such that almost all edges ($>99\%$) occur in at least one induced subgraph. Such criterion is used as smaller $N$ results in smaller subgraphs which we expect to be easier to model accurately. We choose $N=3$ for arXiv for the same reason.

\subsection{Results}  \label{sec:results}

\input{figures/metrics}

Our evaluation results reveal that \name produces both semantically and structurally more accurate ontologies than our baselines. Inspecting the metrics for the Wikipedia task in \cref{table:metrics}, we see that although \name is outperformed by the \textbf{Memorisation} and \textbf{Finetune} on Literal F1, it is much better at the Fuzzy, Continuous and Graph F1 metrics. This suggests that while \name produces ontologies that are \emph{syntactically} less aligned to the ground truth, it better captures the overall semantics. In fact, our prompting baselines following the same task format as \name also outperform \textbf{Hearst} and \textbf{REBEL} in the semantics-aware metrics, though they suffer in structural integrity as reflected by the high Motif Distance. The results also hint at the potential pitfalls of syntax-based evaluation metrics as we see syntactic similarity does not generally entail semantic similarity.

The arXiv task differs from the Wikipedia task as it has much fewer relations and there is even less overlap between the train and test split. This imposes a great challenge on \textbf{Finetune} and \name as they need to generalise with a limited diversity of training samples. Despite such constraints, \name is substantially better than other methods in modelling the semantics of the test graph. Inspecting the generated outputs, we observe prompting baselines tend to produce repetitive concepts such as ``Machine Learning and Artificial Intelligence'' and ''Artificial Intelligence and Machine Learning'' while \textbf{Hearst} and \textbf{REBEL} put ``Machine Learning'' as the parent concept of almost all ground truth concepts. Plots for the generated graphs can be found in \cref{appendix:visualisation}.

