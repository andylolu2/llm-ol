\chapter{Evaluation}

\section{Experiments}

We design our experiments to answer the following research questions:
\begin{enumerate}
    \item Does \name produce better ontologies than traditional methods by subtask composition?
    \item Can \name be easily adapted to a new domain?
\end{enumerate}
We approach the questions by training \name on the Wikipedia dataset and further transfer the model to arXiv with a small number of arXiv samples. As baselines, we use two relation extraction methods, Hearst patterns \cite{hearst1998automated,roller2018hearst} and REBEL \cite{cabot2021rebel}. Relation extraction depends on successful concept discovery to produce high-quality ontologies. To estimate a ceiling to such baselines, \emph{we give the baselines a substantial advantage} by providing them with the ground truth concepts in the test graph. The results show that even with such an advantage, \name outperforms the baselines on many metrics, demonstrating the potential of \name for end-to-end OL.

\subsection{Implementation details}  \label{sec:implementation}


The model is trained on the Wikipedia dataset for two epochs with Adam. During inference, the outputs are generated with temperature 0.1 and nucleus sampling~\cite{holtzman2019curious} top-$p$ of 0.9. We include a finetuning baseline without the masked loss objective, denoted as Finetune. To adapt \name for arXiv, we further finetune the model on 2048 document-subgraph pairs from arXiv. We initialise new low-rank adaptors and train until the loss stops improving on the validation set. We name these models \name (transfer) and Finetune (transfer) for training with and without the masked loss objective, respectively. Full details for the Wikipedia and arXiv experiments can be found in~\cref{appendix:training-details}.


The hyperparameters for the post-processing steps are tuned by grid search on the validation set. We sweep over $\alpha \in 1 - \text{geomspace}(1 / |E_\text{raw}|, 1, 21)$ and $\beta \in \text{geomspace}(0.1, 1, 21) - 0.1$ and use the values that maximise Continuous~F1. For Wikipedia, we choose the subgraph modelling path length $N=4$ as it is the smallest $N$ such that almost all edges ($>99\%$) occur in at least one relevant subgraph. Such criterion is used since smaller $N$ results in smaller subgraphs, which we expect to be easier to model accurately. We choose $N=3$ for arXiv for the same reason.


\subsection{Results}  \label{sec:results}

\input{figures/metrics}

We first evaluate whether \name can accurately create ontologies with many concepts and relations, such as the Wikipedia categories. Computationally, \name required 12 A100-hours for training and 7 A100-hours for inference to generate an ontology for Wikipedia. This is a modest cost in current standards, which demonstrates the scalability of \name for real-world problems.
In terms of performance, \name produces the most semantically accurate ontology in comparison to our baselines as presented in \cref{table:metrics}. Across all of Fuzzy~F1, Continuous~F1 and Graph~F1, we observe the trend that \name scores the best, followed by Finetune and Prompting, and lastly Hearst and REBEL. This is surprising, as it suggests that the combination of LLMs with our subgraph modelling framework is a sufficiently strong inductive bias for LLMs to outperform traditional methods even without finetuning. However, prompting alone is not sufficient to build high-quality ontologies.
On the Motif Distance metric, prompting methods score poorly at 0.314--0.354 in comparison to 0.050 and 0.080 for Finetune and \name respectively. This shows that using LLMs out-of-the-box for subgraph modelling results in poor structural integrity, though this issue is solved by finetuning.
Qualitatively, we observe that \name can adhere to the clear, explicit naming style of Wikipedia, even on unseen topics in the test set. For example, it generates ``Mathematical categories'' and ``Groups (mathematics)'' under the parent concept ``Mathematical structures'' to distinguish from the natural language sense of categories and groups (\cref{fig:ollm-wiki-samples-math}). Such style is not learned by the prompting baselines: Three-shot generated ``Elections $\to$ France'', while it most likely meant ``Elections $\to$ Elections in France'' (\cref{fig:3shot-wiki-samples-election}). More sample outputs are shown in \cref{appendix:viz-wiki}.

The arXiv task differs from the Wikipedia task as it has much fewer relations, and there is even less overlap between the train and test split. This imposes a great challenge on Finetune and \name as they need to generalise with a limited diversity of training samples. Despite such constraints, \name is substantially better than other methods in modelling the semantics of the test graph.
On the Fuzzy~F1, Continuous~F1, and Graph~F1 metrics, \name performs the best among all methods with 0.570, 0.357, and 0.633, significantly higher than the next-best of 0.460, 0.290 and 0.546 respectively.
Inspecting the generated ontologies (\cref{appendix:viz-arxiv}), we observe that prompting baselines tend to produce repetitive concepts such as ``Machine Learning and Artificial Intelligence'' and ''Artificial Intelligence and Machine Learning'' (\cref{fig:3shot-arxiv}), while Hearst and REBEL put almost all concepts under the same parent concept(s) (\cref{fig:hearst-arxiv,fig:rebel-arxiv}).
We also found that \name's output for arXiv contains concepts from Wikipedia, but restructured in a way that fits the arXiv ontology. For example, ``Life sciences'' and ``Biological evolution'' appear in the Wikipedia training set under the same parent category ``Life'' with no direct links between them. On the generated graph for arXiv, ``Life sciences'' is instead promoted to one of the top-level concepts with ``Biological Evolution'' as one of its children, which better fits the ``fields of science'' style of the arXiv ontology (\cref{fig:ollm-arxiv}). This demonstrates that \name can adapt to produce a new type of ontology by restructuring its learned concepts, all using just a small number of training samples.

In summary, \name scores the best or is competitive across all metrics in both tasks, with the notable exception of the Literal~F1 metric. We attribute this to the fact that Literal~F1 is sensitive to factors like casing and choice of words, and generally only measures syntactic similarity. For example, we see that a suboptimal baseline like Memorisation scores the best on this metric with 0.134 on the Wikipedia task. This reflects that syntactic similarity generally does not entail semantic similarity so syntax-based metrics should not be used as stand-alone measures for ontology quality.
