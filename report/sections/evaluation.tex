\chapter{Evaluation}

In this chapter, we aim to answer our research questions:
\begin{enumerate}[leftmargin=*]
    \item Does \name produce better ontologies than traditional methods by subtask composition?
    \item Can \name be easily adapted to a new domain?
\end{enumerate}
Since our problem setup is uncommon in existing literature, we also develop new evaluation methods. Evaluating a quality of an ontolgoy is a hard problem as there are no quantitative definitions of what constitutes a ``good ontology'', and metrics generally only capture one aspect (e.g., structure but not semantics) of an ontology. We approach evaluation by treating the ground truth as a proxy for a good ontology, and comparing the generated ontologies against the ground truth.

In \cref{sec:metrics}, we introduce new metrics that are more robust for measuring ontology similarity. We design our experiments to answer the research questions in \cref{sec:experiments}, and present the results in \cref{sec:results}. We show that \name outperforms all of our baselines and can be easily adapted to a new domain using a small number of training samples, demonstrating the potential of \name for end-to-end OL.

\section{Metrics}  \label{sec:metrics}

Many existing methods for comparing ontologies rely on syntactic measures like string edit distance~\cite{Ehrig2005SimilarityFO} as a proxy for semantic similarity, or require every concept to be tagged with descriptions or documents for distributional semantics comparison~\cite{Zavitsanos2011GoldSE}. To obtain more robust and general evaluation results, we introduce a suite of similarity metrics that use modern methods like text embeddings~\cite{reimers-2019-sentence-bert}. Multiple metrics are used as they trade off between interpretability and comprehensiveness, and we aim to make them complementary by capturing different aspects of an ontology. In this section, we denote the ground truth ontology graph as $G = (V, E)$ and the generated graph as $G' = (V', E')$.

\paragraph{Literal~F1}
While literal text matching is unreliable, it is also the simplest and the most interpretable. We treat this metric as a reference metric for sanity check. The Literal~F1 metric~\cite{Kashyap2005TaxaMinerAE} is given by the harmonic mean of the precision and recall of the edges:
\[
    P_{\text{literal}} = \frac{|E \cap E'|}{|E'|} \qquad
    R_{\text{literal}} = \frac{|E \cap E'|}{|E|} \qquad
    \text{Literal~F1} = \left(\frac{1}{P_{\text{literal}}} + \frac{1}{R_{\text{literal}}}\right)^{-1}
\]

\paragraph{Fuzzy~F1}
The Literal~F1 metric puts a strong emphasis on using the correct wording, while in practice, we are interested in evaluating the semantics of an ontology. For example, using a synonymous phrase for a concept should not be penalised. The current state-of-the-art for semantic textual similarity tasks are sentence transformers \cite{reimers-2019-sentence-bert,jiang2022improved,jiang2019smart}. On a high level, a sentence transformer is an embedding function $f: \text{string} \to \mathbb{R}^d$ that maps a string to a $d$-dimensional vector space such that semantically similar strings are close in the vector space, usually measured by cosine similarity:
\[
    \nodesim(u, u') = \frac{f(u) \cdot f(u')}{\|f(u)\| \|f(u')\|}
\]
We consider two nodes $u$ and $u'$ as a fuzzy match if $\nodesim(u, u') > t$, where $t$ is a threshold, chosen to be the median cosine similarity between the synonyms in WordNet~\cite{miller1995wordnet} (computed as 0.436). Two edges $(u, v)$ and $(u', v')$ are a fuzzy match if both $u$ and $v$ are fuzzy matches with $u'$ and $v'$ respectively. The Fuzzy~F1 score is given by the precision and recall of the fuzzy matches:
\begin{equation*}
    \begin{aligned}
        P_{\text{fuzzy}} & = \frac{|
            \{(u', v') \in E' \mid \exists (u, v) \in E.
            \nodesim(u, u') > t \land \nodesim(v, v') > t
            \}
        |}{|E'|}                                                                                       \\
        R_{\text{fuzzy}} & = \frac{|
            \{(u, v) \in E \mid \exists (u', v') \in E'.
            \nodesim(u, u') > t \land \nodesim(v, v') > t
            \}
        |}{|E|}                                                                                        \\
        \text{Fuzzy~F1}  & = \left(\frac{1}{P_{\text{fuzzy}}} + \frac{1}{R_{\text{fuzzy}}}\right)^{-1}
    \end{aligned}
\end{equation*}
The Fuzzy~F1 metric can be seen as a generalisation of the Literal~F1 metric: we recover the Literal~F1 metric if $t = 1$. We use all-MiniLM-L6-v2~\cite{wang2020minilm,reimers-2019-sentence-bert} as the embedding model.

\paragraph{Continuous~F1}
With fuzzy comparisons, the matches between the edges of the generated and the ground truth graph are no longer one-to-one. This is problematic when there are repetitive elements:
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[>=latex]
        \sffamily
        \newcommand{\dist}{1.5cm}
        \node (A1) {A};
        \node (B1) at ($(A1) + (0, -\dist)$) {B};
        \draw[->] (A1) -- (B1);

        \node (A2) at ($(A1) + (2.5, 0)$) {A};
        \node (B2) at ($(A2) + (-0.8, -\dist)$) {B};
        \node (B2') at ($(A2) + (0.8, -\dist)$) {B'};
        \draw[->] (A2) -- (B2);
        \draw[->] (A2) -- (B2');
    \end{tikzpicture}
\end{figure}
\vspace{-1em}
where $B$ and $B'$ match fuzzily. This pair of graphs achieves a perfect Fuzzy~F1 score, yet they are clearly different. Additionally, we found that the discrete nature of Fuzzy F1 makes it unable to provide useful signals for hyperparameter tuning, particularly for our baselines where the generated graphs are poor. The Continuous~F1 metric solves these issues by computing the highest-scoring edge matching between the two graphs, where the similarity score between $(u, v)$ and $(u', v')$ is given by $\min(\nodesim(u, u'), \nodesim(v, v'))$. We choose to use $\min(\nodesim(u, u'), \nodesim(v, v'))$ as the similarity score to ensure that only edges where both nodes partially match (i.e. cosine similarity $> 0$) can contribute to the total matching score. Obtaining such matching is equivalent to solving the linear assignment problem~\cite{martello1987linear}, which can be computed by the Hungarian algorithm~\cite{kuhn1955hungarian}. Given the score $s_{\text{cout}}$ achieved by the best edge matching, the Continuous~F1 score is obtained from the continuous precision and recall, given by:
\[
    P_{\text{cont}} = \frac{s_{\text{cout}}}{|E'|} \qquad
    R_{\text{cont}} = \frac{s_{\text{cout}}}{|E|} \qquad
    \text{Continuous~F1} = \left(\frac{1}{P_{\text{cont}}} + \frac{1}{R_{\text{cont}}}\right)^{-1}
\]

\paragraph{Graph~F1}
Instead of comparing individual edges, this metric aims to capture the wider structure of the two graphs. Intuitively, we want to know how concepts are related to their local neighbourhood. A well known method for capturing graph structure in vectors is node embeddings, as discussed in \cref{sec:node-embeddings}. Concretely, we use simple graph convolutions \cite{wu2019simplifying} with identity weights and $K=2$ as a parameter-free method to compute graph-aware node representations after embedding each node with the pretrained embedder. Such embeddings in $G$ are compared against those in $G'$ by cosine similarity, and the highest-scoring node matching, similar to the Continuous~F1 metric, gives the graph similarity score $s_{\text{graph}}$. The Graph~F1 score is computed from the graph precision and recall, defined as:
\[
    P_{\text{graph}} = \frac{s_{\text{graph}}}{|V'|} \qquad
    R_{\text{graph}} = \frac{s_{\text{graph}}}{|V|} \qquad
    \text{Graph~F1} = \left(\frac{1}{P_{\text{graph}}} + \frac{1}{R_{\text{graph}}}\right)^{-1}
\]

\paragraph{Motif distance}
Taking inspiration from classical network analysis, we use \emph{network motifs}~\cite{milo2002network,shen2002network} to evaluate the structural integrity of the generated graphs. Network motifs are reoccurring subgraphs in a larger graph, most commonly 3-vertex subgraphs. They are typically indicative of the structural characteristics of the full graph. We define the motif distance as the 1-Wasserstein distance~\cite{Kantorovich1960MathematicalMO} between the distribution of all 3-vertex subgraphs in $G$ and $G'$.


\section{Experiments}  \label{sec:experiments}

We design our experiments to answer the following research questions:
\begin{enumerate}
    \item Does \name produce better ontologies than traditional methods by subtask composition?
    \item Can \name be easily adapted to a new domain?
\end{enumerate}
We approach the questions by training \name on the Wikipedia dataset and further transfer the model to arXiv with a small number of arXiv samples. As baselines, we use two relation extraction methods, Hearst patterns \cite{hearst1998automated,roller2018hearst} and REBEL \cite{cabot2021rebel}. Relation extraction depends on successful concept discovery to produce high-quality ontologies. To estimate a ceiling to such baselines, \emph{we give the baselines a substantial advantage} by providing them with the ground truth concepts in the test graph. The results show that even with such an advantage, \name outperforms the baselines on many metrics, demonstrating the potential of \name for end-to-end OL.

\subsection{Implementation details}  \label{sec:implementation}


The model is trained on the Wikipedia dataset for two epochs with Adam. During inference, the outputs are generated with temperature 0.1 and nucleus sampling~\cite{holtzman2019curious} top-$p$ of 0.9. We include a finetuning baseline without the masked loss objective, denoted as Finetune. To adapt \name for arXiv, we further finetune the model on 2048 document-subgraph pairs from arXiv. We initialise new low-rank adaptors and train until the loss stops improving on the validation set. We name these models \name (transfer) and Finetune (transfer) for training with and without the masked loss objective, respectively. Full details for the Wikipedia and arXiv experiments can be found in~\cref{appendix:training-details}.


The hyperparameters for the post-processing steps are tuned by grid search on the validation set. We sweep over $\alpha \in 1 - \text{geomspace}(1 / |E_\text{raw}|, 1, 21)$ and $\beta \in \text{geomspace}(0.1, 1, 21) - 0.1$ and use the values that maximise Continuous~F1. For Wikipedia, we choose the subgraph modelling path length $N=4$ as it is the smallest $N$ such that almost all edges ($>99\%$) occur in at least one relevant subgraph. Such criterion is used since smaller $N$ results in smaller subgraphs, which we expect to be easier to model accurately. We choose $N=3$ for arXiv for the same reason.


\section{Results}  \label{sec:results}

\input{figures/metrics}

We first evaluate whether \name can accurately create ontologies with many concepts and relations, such as the Wikipedia categories. Computationally, \name required 12 A100-hours for training and 7 A100-hours for inference to generate an ontology for Wikipedia. This is a modest cost in current standards, which demonstrates the scalability of \name for real-world problems.
In terms of performance, \name produces the most semantically accurate ontology in comparison to our baselines as presented in \cref{table:metrics}. Across all of Fuzzy~F1, Continuous~F1 and Graph~F1, we observe the trend that \name scores the best, followed by Finetune and Prompting, and lastly Hearst and REBEL. This is surprising, as it suggests that the combination of LLMs with our subgraph modelling framework is a sufficiently strong inductive bias for LLMs to outperform traditional methods even without finetuning. However, prompting alone is not sufficient to build high-quality ontologies.
On the Motif Distance metric, prompting methods score poorly at 0.314--0.354 in comparison to 0.050 and 0.080 for Finetune and \name respectively. This shows that using LLMs out-of-the-box for subgraph modelling results in poor structural integrity, though this issue is solved by finetuning.
Qualitatively, we observe that \name can adhere to the clear, explicit naming style of Wikipedia, even on unseen topics in the test set. For example, it generates ``Mathematical categories'' and ``Groups (mathematics)'' under the parent concept ``Mathematical structures'' to distinguish from the natural language sense of categories and groups (\cref{fig:ollm-wiki-samples-math}). Such style is not learned by the prompting baselines: Three-shot generated ``Elections $\to$ France'', while it most likely meant ``Elections $\to$ Elections in France'' (\cref{fig:3shot-wiki-samples-election}). More sample outputs are shown in \cref{appendix:viz-wiki}.

The arXiv task differs from the Wikipedia task as it has much fewer relations, and there is even less overlap between the train and test split. This imposes a great challenge on Finetune and \name as they need to generalise with a limited diversity of training samples. Despite such constraints, \name is substantially better than other methods in modelling the semantics of the test graph.
On the Fuzzy~F1, Continuous~F1, and Graph~F1 metrics, \name performs the best among all methods with 0.570, 0.357, and 0.633, significantly higher than the next-best of 0.460, 0.290 and 0.546 respectively.
Inspecting the generated ontologies (\cref{appendix:viz-arxiv}), we observe that prompting baselines tend to produce repetitive concepts such as ``Machine Learning and Artificial Intelligence'' and ''Artificial Intelligence and Machine Learning'' (\cref{fig:3shot-arxiv}), while Hearst and REBEL put almost all concepts under the same parent concept(s) (\cref{fig:hearst-arxiv,fig:rebel-arxiv}).
We also found that \name's output for arXiv contains concepts from Wikipedia, but restructured in a way that fits the arXiv ontology. For example, ``Life sciences'' and ``Biological evolution'' appear in the Wikipedia training set under the same parent category ``Life'' with no direct links between them. On the generated graph for arXiv, ``Life sciences'' is instead promoted to one of the top-level concepts with ``Biological Evolution'' as one of its children, which better fits the ``fields of science'' style of the arXiv ontology (\cref{fig:ollm-arxiv}). This demonstrates that \name can adapt to produce a new type of ontology by restructuring its learned concepts, all using just a small number of training samples.

In summary, \name scores the best or is competitive across all metrics in both tasks, with the notable exception of the Literal~F1 metric. We attribute this to the fact that Literal~F1 is sensitive to factors like casing and choice of words, and generally only measures syntactic similarity. For example, we see that a suboptimal baseline like Memorisation scores the best on this metric with 0.134 on the Wikipedia task. This reflects that syntactic similarity generally does not entail semantic similarity so syntax-based metrics should not be used as stand-alone measures for ontology quality.
