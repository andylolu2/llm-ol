{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import HTML\n",
    "\n",
    "from llm_ol.experiments.llm.templates import PROMPT_TEMPLATE, RESPONSE_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "model_id = \"out/experiments/finetune/v4/train/checkpoint-final/merged\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Example input text\n",
    "with open(\"out/experiments/llm/v2/test_dataset.jsonl\") as f:\n",
    "    examples = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# example_idx = random.randint(0, len(examples) - 1)\n",
    "# print(f\"Example index: {example_idx}\")\n",
    "example_idx = 75689\n",
    "example = examples[example_idx]\n",
    "prompt = PROMPT_TEMPLATE.render(title=example[\"title\"], abstract=example[\"abstract\"])\n",
    "response = RESPONSE_TEMPLATE.render(paths=example[\"paths\"])\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "    {\"role\": \"assistant\", \"content\": response},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True).to(\n",
    "    model.device\n",
    ")\n",
    "\n",
    "input_ids = inputs.input_ids.to(model.device)\n",
    "\n",
    "inst_end = [733, 28748, 16289, 28793]  # _[/INST]\n",
    "\n",
    "\n",
    "def find_index(list_, sublist):\n",
    "    for i in range(len(list_) - len(sublist) + 1):\n",
    "        if list_[i : i + len(sublist)] == sublist:\n",
    "            return i\n",
    "    raise ValueError(f\"Sublist {sublist} not found in list\")\n",
    "\n",
    "\n",
    "resp_start_idx = find_index(input_ids[0].tolist(), inst_end) + len(inst_end)\n",
    "\n",
    "# Forward pass to compute logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids)\n",
    "\n",
    "# Compute per-token loss\n",
    "logits = outputs.logits[:, :-1]\n",
    "labels = input_ids[:, 1:]\n",
    "loss = torch.nn.functional.cross_entropy(\n",
    "    logits.view(-1, logits.shape[-1]), labels.view(-1), reduction=\"none\"\n",
    ")\n",
    "loss = loss.view(labels.shape)\n",
    "loss[:, :resp_start_idx] = 0  # Ignore loss for prompt\n",
    "\n",
    "# Normalize loss values\n",
    "print(loss.max())\n",
    "# normalized_loss = (loss - torch.min(loss)) / (torch.max(loss) - torch.min(loss))\n",
    "normalized_loss = loss / 25\n",
    "normalized_loss = normalized_loss.cpu()[0].tolist()\n",
    "normalized_loss = [0] + normalized_loss  # Add loss for first token\n",
    "\n",
    "html_pre = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "<div style=\"font-family: monospace; width: 1000px; background-color: white; padding: 10px; color: black;\">\n",
    "\"\"\"\n",
    "html_post = \"\"\"\n",
    "</div>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "html_body = \"\"\n",
    "for i, (color, (start, end)) in enumerate(\n",
    "    zip(normalized_loss, inputs[\"offset_mapping\"][0].tolist())\n",
    "):\n",
    "    # escape\n",
    "    chars = text[start:end]\n",
    "    if i == resp_start_idx:\n",
    "        chars = \"\\n\" + chars\n",
    "    chars = chars.replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n",
    "    # replace newlines with <br>\n",
    "    chars = chars.replace(\"\\n\", \"<br>\")\n",
    "    html_body += (\n",
    "        f'<span style=\"background-color: rgba(255, 0, 0, {color});\">{chars}</span>'\n",
    "    )\n",
    "\n",
    "html = HTML(html_pre + html_body + html_post)\n",
    "display(html)\n",
    "\n",
    "with open(f\"out/graphs/loss_masked_{example_idx}.html\", \"w\") as f:\n",
    "    f.write(html.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3855 105707 51421 75689 86575"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-ol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
