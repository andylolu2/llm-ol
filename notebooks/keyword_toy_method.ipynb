{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    TokenClassificationPipeline,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from transformers.pipelines import AggregationStrategy\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "from llm_ol.dataset import data_model\n",
    "\n",
    "torch.set_default_device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define keyphrase extraction pipeline\n",
    "class KeyphraseExtractionPipeline(TokenClassificationPipeline):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__(\n",
    "            model=AutoModelForTokenClassification.from_pretrained(model_name),\n",
    "            tokenizer=AutoTokenizer.from_pretrained(model_name),\n",
    "            aggregation_strategy=AggregationStrategy.SIMPLE,\n",
    "            device=0 if torch.cuda.is_available() else -1,\n",
    "        )\n",
    "\n",
    "    def postprocess(self, *args, **kwargs):\n",
    "        results = super().postprocess(*args, **kwargs)\n",
    "        return np.unique([result.get(\"word\").strip() for result in results])\n",
    "\n",
    "\n",
    "# Load pipeline\n",
    "model_name = \"ml6team/keyphrase-extraction-kbir-inspec\"\n",
    "extractor = KeyphraseExtractionPipeline(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path(\"out/data/wikipedia/v1/full/full_graph.json\")\n",
    "\n",
    "G = data_model.load_graph(file_path, depth=1)\n",
    "seen = set()\n",
    "titles = []\n",
    "abstracts = []\n",
    "for _, data in G.nodes(data=True):\n",
    "    for page in data[\"pages\"]:\n",
    "        if page[\"title\"] in seen:\n",
    "            continue\n",
    "        seen.add(page[\"title\"])\n",
    "        titles.append(page[\"title\"])\n",
    "        abstracts.append(page[\"abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrases = set()\n",
    "\n",
    "for abstract in abstracts:\n",
    "    result = extractor(abstract)\n",
    "    for keyphrase in result:\n",
    "        keyphrases.add(keyphrase.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out/data/wikipedia/v1/full/keyphrases.txt\", \"w\") as f:\n",
    "    for keyphrase in keyphrases:\n",
    "        f.write(keyphrase + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "\n",
    "class HearstPattern:\n",
    "    def findall(self, text: str) -> list[tuple[str, str]]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SuchAsPattern(HearstPattern):\n",
    "    def __init__(self):\n",
    "        self.pattern = re.compile(\n",
    "            r\"(?P<src>NP_[\\w'-]+),? such as ((?P<tgt>NP_[\\w'-]+)( |,|and|or)*)+\"\n",
    "        )\n",
    "\n",
    "    def findall(self, text: str):\n",
    "        for m in self.pattern.finditer(text):\n",
    "            src = m.group(\"src\")\n",
    "            for tgt in m.captures(\"tgt\"):\n",
    "                yield src, tgt\n",
    "\n",
    "\n",
    "class AsPattern(HearstPattern):\n",
    "    def __init__(self):\n",
    "        self.pattern = re.compile(\n",
    "            r\"(?P<src>NP_[\\w'-]+),? as ((?P<tgt>NP_[\\w'-]+)( |,|and|or)*)+\"\n",
    "        )\n",
    "\n",
    "    def findall(self, text: str):\n",
    "        for m in self.pattern.finditer(text):\n",
    "            src = m.group(\"src\")\n",
    "            for tgt in m.captures(\"tgt\"):\n",
    "                yield src, tgt\n",
    "\n",
    "\n",
    "class AndOtherPattern(HearstPattern):\n",
    "    def __init__(self):\n",
    "        self.pattern = re.compile(\n",
    "            r\"((?P<tgt>NP_[\\w'-]+)( |,|and|or)*)+(and|or) other (?P<src>NP_[\\w'-]+)\",\n",
    "        )\n",
    "\n",
    "    def findall(self, text: str):\n",
    "        for m in self.pattern.finditer(text):\n",
    "            src = m.group(\"src\")\n",
    "            for tgt in m.captures(\"tgt\"):\n",
    "                yield src, tgt\n",
    "\n",
    "\n",
    "class IncludePattern(HearstPattern):\n",
    "    def __init__(self):\n",
    "        self.pattern = re.compile(\n",
    "            r\"(?P<src>NP_[\\w'-]+),? include ((?P<tgt>NP_[\\w'-]+)( |,|and|or)*)+\"\n",
    "        )\n",
    "\n",
    "    def findall(self, text: str):\n",
    "        for m in self.pattern.finditer(text):\n",
    "            src = m.group(\"src\")\n",
    "            for tgt in m.captures(\"tgt\"):\n",
    "                yield src, tgt\n",
    "\n",
    "\n",
    "class EspeciallyPattern(HearstPattern):\n",
    "    def __init__(self):\n",
    "        self.pattern = re.compile(\n",
    "            r\"(?P<src>NP_[\\w'-]+),? especially ((?P<tgt>NP_[\\w'-]+)( |,|and|or)*)+\"\n",
    "        )\n",
    "\n",
    "    def findall(self, text: str):\n",
    "        for m in self.pattern.finditer(text):\n",
    "            src = m.group(\"src\")\n",
    "            for tgt in m.captures(\"tgt\"):\n",
    "                yield src, tgt\n",
    "\n",
    "\n",
    "HEARST_PATTERNS = [\n",
    "    r\"(NP_\\w+,? such as (NP_\\w+ ?(, )?(and |or )?)+)\",\n",
    "    r\"(such NP_\\w+,? as (NP_\\w+ ?(, )?(and |or )?)+)\",\n",
    "    r\"((NP_\\w+ ?(, )?)+(and |or )?other NP_\\w+)\",\n",
    "    r\"(NP_\\w+,? include (NP_\\w+ ?(, )?(and |or )?)+)\",\n",
    "    r\"(NP_\\w+,? especially (NP_\\w+ ?(, )?(and |or )?)+)\",\n",
    "    # r\"((NP_\\w+ ?(, )?)+(and |or )?any other NP_\\w+)\",\n",
    "    # r\"((NP_\\w+ ?(, )?)+(and |or )?some other NP_\\w+)\",\n",
    "    # r\"((NP_\\w+ ?(, )?)+(and |or )?be a NP_\\w+)\",\n",
    "    # r\"(NP_\\w+,? like (NP_\\w+ ?,? (and |or )?)*NP_\\w)\",\n",
    "    # r\"such (NP_\\w+,? as (NP_\\w+ ?,? (and |or )?)+)\",\n",
    "    # r\"((NP_\\w+ ?(, )?)+(and |or )?like other NP_\\w+)\",\n",
    "    # r\"((NP_\\w+ ?(, )?)+(and |or )?one of the NP_\\w+)\",\n",
    "    # r\"((NP_\\w+ ?(, )?)+(and |or )?one of these NP_\\w+)\",\n",
    "    # r\"((NP_\\w+ ?(, )?)+(and |or )?one of those NP_\\w+)\",\n",
    "    # r\"example of (NP_\\w+,? be (NP_\\w+ ?,? (and |or )?)+)\",\n",
    "    # r\"((NP_\\w+ ?(, )?)+(and |or )?be example of NP_\\w+)\",\n",
    "    # r\"(NP_\\w+,? for example,? (NP_\\w+ ?(, )?(and |or )?)+)\",\n",
    "    # r\"((NP_\\w+ ?(, )?)+(and |or )?which be call NP_\\w+)\",\n",
    "    # r\"((NP_\\w+ ?(, )?)+(and |or )?which be name NP_\\w+)\",\n",
    "    # r\"(NP_\\w+,? mainly (NP_\\w+ ?,? (and |or )?)+)\",\n",
    "    # r\"(NP_\\w+,? mostly (NP_\\w+ ?,? (and |or )?)+)\",\n",
    "    # r\"(NP_\\w+,? notably (NP_\\w+ ?,? (and |or )?)+)\",\n",
    "    # r\"(NP_\\w+,? particularly (NP_\\w+ ?,? (and |or )?)+)\",\n",
    "    # r\"(NP_\\w+,? principally (NP_\\w+ ?,? (and |or )?)+)\",\n",
    "    # r\"(NP_\\w+,? in particular (NP_\\w+ ?,? (and |or )?)+)\",\n",
    "    # r\"(NP_\\w+,? except (NP_\\w+ ?,? (and |or )?)+)\",\n",
    "    # r\"(NP_\\w+,? other than (NP_\\w+ ?,? (and |or )?)+)\",\n",
    "    # r\"(NP_\\w+,? e.g.,? (NP_\\w+ ?,? (and |or )?)+)\",\n",
    "    # r\"(NP_\\w+ \\( (e.g.|i.e.),? (NP_\\w+ ?,? (and |or )?)+(\\. )?\\))\",\n",
    "    # r\"(NP_\\w+,? i.e.,? (NP_\\w+ ?,? (and |or )?)+)\",\n",
    "    # r\"((NP_\\w+ ?(, )?)+(and|or)? a kind of NP_\\w+)\",\n",
    "    # r\"((NP_\\w+ ?(, )?)+(and|or)? kind of NP_\\w+)\",\n",
    "    # r\"((NP_\\w+ ?(, )?)+(and|or)? form of NP_\\w+)\",\n",
    "    # r\"((NP_\\w+ ?(, )?)+(and |or )?which look like NP_\\w+)\",\n",
    "    # r\"((NP_\\w+ ?(, )?)+(and |or )?which sound like NP_\\w+)\",\n",
    "    # r\"(NP_\\w+,? which be similar to (NP_\\w+ ?,? (and |or )?)+)\",\n",
    "    # r\"(NP_\\w+,? example of this be (NP_\\w+ ?,? (and |or )?)+)\",\n",
    "    # r\"(NP_\\w+,? type (NP_\\w+ ?,? (and |or )?)+)\",\n",
    "    # r\"((NP_\\w+ ?(, )?)+(and |or )? NP_\\w+ type)\",\n",
    "    # r\"(NP_\\w+,? whether (NP_\\w+ ?,? (and |or )?)+)\",\n",
    "    # r\"(compare (NP_\\w+ ?(, )?)+(and |or )?with NP_\\w+)\",\n",
    "    # r\"(NP_\\w+,? compare to (NP_\\w+ ?,? (and |or )?)+)\",\n",
    "    # # r\"(NP_\\w+,? among -PRON- (NP_\\w+ ?,? (and |or )?)+)\",\n",
    "    # # r\"((NP_\\w+ ?(, )?)+(and |or )?as NP_\\w+)\",  <-- bad\n",
    "    # r\"(NP_\\w+,?  (NP_\\w+ ?,? (and |or )?)+ for instance)\",\n",
    "    # r\"((NP_\\w+ ?(, )?)+(and|or)? sort of NP_\\w+)\",\n",
    "    # r\"(NP_\\w+,? which may include (NP_\\w+ ?(, )?(and |or )?)+)\",\n",
    "]\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n",
    "nlp.add_pipe(\"merge_noun_chunks\")\n",
    "# ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "# ruler.add_patterns(\n",
    "#     [\n",
    "#         {\n",
    "#             \"label\": \"NP\",\n",
    "#             \"pattern\": [\n",
    "#                 # {\"POS\": \"PRON\", \"OP\": \"{0,1}\"},\n",
    "#                 # {\"POS\": \"ADJ\", \"OP\": \"*\"},\n",
    "#                 # {\"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}, \"OP\": \"+\"},\n",
    "#                 {\"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}},\n",
    "#             ],\n",
    "#         }\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# matcher = Matcher(nlp.vocab)\n",
    "# matcher.add(\n",
    "#     \"NP is NP\",\n",
    "#     [\n",
    "#         [\n",
    "#             {\"ENT_TYPE\": \"NP\"},\n",
    "#             {\"LOWER\": \"such\"},\n",
    "#             {\"LOWER\": \"as\"},\n",
    "#             {\"ENT_TYPE\": \"NP\", \"OP\": \"+\"},\n",
    "#             {\"ENT_TYPE\": \"NP\"},\n",
    "#         ],\n",
    "#     ],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, Tuple, Union\n",
    "\n",
    "from spacy.tokens import Doc, Span\n",
    "from spacy.symbols import NOUN, PROPN, PRON\n",
    "from spacy.errors import Errors\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "\n",
    "def noun_chunks(doclike: Union[Doc, Span]):\n",
    "    \"\"\"\n",
    "    Detect base noun phrases from a dependency parse. Works on both Doc and Span.\n",
    "    \"\"\"\n",
    "    labels = [\n",
    "        \"oprd\",\n",
    "        \"nsubj\",\n",
    "        \"dobj\",\n",
    "        \"nsubjpass\",\n",
    "        \"pcomp\",\n",
    "        \"pobj\",\n",
    "        \"dative\",\n",
    "        \"appos\",\n",
    "        \"attr\",\n",
    "        \"ROOT\",\n",
    "    ]\n",
    "    doc = doclike.doc  # Ensure works on both Doc and Span.\n",
    "    if not doc.has_annotation(\"DEP\"):\n",
    "        raise ValueError(Errors.E029)\n",
    "    np_deps = [doc.vocab.strings.add(label) for label in labels]\n",
    "    conj = doc.vocab.strings.add(\"conj\")\n",
    "    np_label = doc.vocab.strings.add(\"NP\")\n",
    "    prev_end = -1\n",
    "    for i, word in enumerate(doclike):\n",
    "        if word.pos not in (NOUN, PROPN, PRON):\n",
    "            continue\n",
    "        # Prevent nested chunks from being produced\n",
    "        if word.left_edge.i <= prev_end:\n",
    "            continue\n",
    "        if word.dep in np_deps:\n",
    "            prev_end = word.i\n",
    "            left_i = word.left_edge.i\n",
    "            while left_i < word.i and doc[left_i].dep_ in (\"det\", \"poss\", \"case\"):\n",
    "                left_i += 1\n",
    "            yield Span(doc, left_i, word.i + 1, label=np_label)\n",
    "        elif word.dep == conj:\n",
    "            head = word.head\n",
    "            while head.dep == conj and head.head.i < head.i:\n",
    "                head = head.head\n",
    "            # If the head is an NP, and we're coordinated to it, we're an NP\n",
    "            if head.dep in np_deps:\n",
    "                prev_end = word.i\n",
    "                left_i = word.left_edge.i\n",
    "                while left_i < word.i and doc[left_i].dep_ in (\"det\", \"poss\"):\n",
    "                    left_i += 1\n",
    "                yield Span(doc, word.left_edge.i, word.i + 1, label=np_label)\n",
    "\n",
    "\n",
    "def merge_noun_chunks(doc: Doc) -> Doc:\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        nps = list(noun_chunks(doc))\n",
    "        for np in nps:\n",
    "            attrs = {\"tag\": np.root.tag, \"dep\": np.root.dep}\n",
    "            retokenizer.merge(np, attrs=attrs)  # type: ignore[arg-type]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(abstracts[1])\n",
    "doc = merge_noun_chunks(doc)\n",
    "displacy.render(doc, style=\"dep\", jupyter=True, options={\"compact\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "# text = \"Programming languages such as Python, Java, and C++ are popular.\"\n",
    "\n",
    "i = 2\n",
    "\n",
    "patterns = [\n",
    "    SuchAsPattern(),\n",
    "    AsPattern(),\n",
    "    AndOtherPattern(),\n",
    "    IncludePattern(),\n",
    "    EspeciallyPattern(),\n",
    "]\n",
    "\n",
    "\n",
    "def extract_hyponyms(text: str):\n",
    "    doc = nlp(text)\n",
    "    # doc = merge_noun_chunks(doc)\n",
    "\n",
    "    new_text = []\n",
    "    for token in doc:\n",
    "        lemmatized = token.text if token.pos_ in (\"NOUN\", \"PROPN\") else token.lemma_\n",
    "        if token.pos_ in (\"NOUN\", \"PROPN\"):\n",
    "            new_text.append(\"NP_\" + lemmatized.replace(\" \", \"_\"))\n",
    "            # text_with_ws = token.text_with_ws\n",
    "            # if text_with_ws.endswith(\" \"):\n",
    "            #     text_replaced = text_with_ws[:-1].replace(\" \", \"_\") + \" \"\n",
    "            # else:\n",
    "            #     text_replaced = text_with_ws.replace(\" \", \"_\")\n",
    "            # new_text.append(\"NP_\" + text_replaced)\n",
    "        else:\n",
    "            # new_text.append(token.text_with_ws)\n",
    "            new_text.append(lemmatized)\n",
    "        if token.whitespace_:\n",
    "            new_text.append(token.whitespace_)\n",
    "    new_text = \"\".join(new_text)\n",
    "\n",
    "    hyponyms = set()\n",
    "    for pattern in patterns:\n",
    "        for src, tgt in pattern.findall(new_text):\n",
    "            hyponyms.add((src, tgt, pattern.__class__.__name__))\n",
    "    return hyponyms\n",
    "\n",
    "\n",
    "hyponyms = set()\n",
    "for abstract in tqdm.tqdm(abstracts):\n",
    "    for src, tgt, pattern in extract_hyponyms(abstract):\n",
    "        hyponyms.add((src, tgt, pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "nlp_normalized = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def denormalize(text: str):\n",
    "    return text.replace(\"_\", \" \").replace(\"NP \", \"\")\n",
    "\n",
    "\n",
    "categories = list(\n",
    "    {denormalize(src) for src, _, _ in hyponyms}\n",
    "    | {denormalize(tgt) for _, tgt, _ in hyponyms}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(np_tag: str):\n",
    "    text = np_tag.replace(\"_\", \" \").replace(\"NP \", \"\")\n",
    "    doc = nlp_normalized(text)\n",
    "    new_text = []\n",
    "    for token in doc:\n",
    "        if token.dep_ in (\"det\", \"poss\"):\n",
    "            continue\n",
    "        if token.tag_ in (\"NNS\", \"NNPS\"):\n",
    "            new_text.append(token.lemma_.lower())\n",
    "        else:\n",
    "            new_text.append(token.text.lower())\n",
    "        if token.whitespace_:\n",
    "            new_text.append(token.whitespace_)\n",
    "    new_text = \"\".join(new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = [\n",
    "    (normalize(src), normalize(tgt)) for src, tgt, pattern in tqdm.tqdm(hyponyms)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "src_count = len({src for src, _ in relations})\n",
    "tgt_count = len({tgt for _, tgt in relations})\n",
    "rel_count = len({(src, tgt) for src, tgt in relations})\n",
    "print(f\"Unique source: {src_count}\")\n",
    "print(f\"Unique target: {tgt_count}\")\n",
    "print(f\"Unique relations: {rel_count}\")\n",
    "\n",
    "tree = defaultdict(lambda: set())\n",
    "for src, tgt in relations:\n",
    "    tree[src].add(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in nlp(\"including\"):\n",
    "    print(token.text, token.pos_, token.dep_, token.lemma_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-ol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
