{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "import json\n",
    "import dataclasses\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib_venn\n",
    "import graph_tool.all as gt\n",
    "from tqdm import tqdm\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "from llm_ol.dataset.data_model import load_graph\n",
    "from llm_ol.utils.nx_to_gt import nx_to_gt\n",
    "from llm_ol.experiments.post_processing import hp_search, post_process, PostProcessHP\n",
    "from llm_ol.eval.graph_metrics import (\n",
    "    edge_prec_recall_f1,\n",
    "    graph_fuzzy_match,\n",
    "    edge_similarity,\n",
    ")\n",
    "from llm_ol.utils import sized_subplots\n",
    "from metadata import query, query_multiple\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Some utilities\n",
    "\n",
    "fig_dir = Path(\"out\", \"graphs\")\n",
    "\n",
    "\n",
    "def display_graph(G: nx.Graph, layout: str = \"dot\", **kwargs):\n",
    "    relabel_map = {}\n",
    "    for n, data in G.nodes(data=True):\n",
    "        relabel_map[n] = data.get(\"title\", n)\n",
    "    G = nx.relabel_nodes(G, relabel_map)\n",
    "    for n, data in G.nodes(data=True):\n",
    "        data.clear()\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        data.clear()\n",
    "\n",
    "    A = nx.nx_agraph.to_agraph(G)\n",
    "    A.node_attr.update(fontname=\"Helvetica\", fontsize=10, shape=\"plaintext\")\n",
    "    A.graph_attr.update(ratio=\"compress\")\n",
    "    A.edge_attr.update(arrowsize=0.5)\n",
    "    for k, v in kwargs.items():\n",
    "        if k.startswith(\"G\"):\n",
    "            A.graph_attr[k[1:]] = v\n",
    "        elif k.startswith(\"N\"):\n",
    "            A.node_attr[k[1:]] = v\n",
    "        elif k.startswith(\"E\"):\n",
    "            A.edge_attr[k[1:]] = v\n",
    "    A.layout(layout)\n",
    "    return A\n",
    "\n",
    "\n",
    "def nth_level_nodes(G: nx.Graph, n: int):\n",
    "    return nx.descendants_at_distance(G, G.graph[\"root\"], n)\n",
    "\n",
    "\n",
    "def nth_level_edges(G: nx.Graph, n: int):\n",
    "    distances = nx.single_source_shortest_path_length(G, G.graph[\"root\"], cutoff=n)\n",
    "    return {(u, v) for u, v in G.edges() if distances.get(u, None) == n}\n",
    "\n",
    "\n",
    "def title(G, n):\n",
    "    return G.nodes[n].get(\"title\", n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# G = load_graph(\"out/data/wikipedia/v2/full/graph_depth_3.json\")\n",
    "G = load_graph(\"out/data/arxiv/v2/full/full_graph.json\")\n",
    "pages = set()\n",
    "for n, data in G.nodes(data=True):\n",
    "    pages.update([page[\"id\"] for page in data[\"pages\"]])\n",
    "print(G.number_of_nodes(), G.number_of_edges(), len(pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "dataset = \"arxiv/v2\"\n",
    "# exp = query(exp=\"finetune\", dataset=dataset, reweighted=True, transfer=True)\n",
    "# exp = query(exp=\"prompting\", dataset=dataset, k_shot=0)\n",
    "# exp = query(exp=\"hearst\", dataset=dataset)\n",
    "# exp = query(exp=\"rebel\", dataset=dataset)\n",
    "exp = query(exp=\"memorisation\", dataset=dataset)\n",
    "G = load_graph(exp.test_output)\n",
    "G, _ = post_process(G, PostProcessHP(**exp.best_hp(\"edge_soft_f1\")))\n",
    "\n",
    "# A = display_graph(G, layout=\"sfdp\", Glevels=1, GK=0.6)\n",
    "A = display_graph(G, layout=\"twopi\", Granksep=1.3, Groot=\"Main topic classifications\")\n",
    "display(A)\n",
    "# A.draw(str(fig_dir / f\"{dataset.replace('/', '_')}_{exp.name}_graph.pdf\"))\n",
    "\n",
    "\n",
    "# G = load_graph(exp.test_ground_truth)\n",
    "# A = display_graph(G, layout=\"fdp\", Goverlap=True, GK=0.8)\n",
    "# A = display_graph(G, layout=\"twopi\", Granksep=1.5, Groot=\"Main topic classifications\")\n",
    "# display(A)\n",
    "# A.draw(str(fig_dir / f\"{dataset.replace('/', '_')}_test_gt_graph.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## arXiv thumbnail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "G = load_graph(\"out/experiments/finetune/arxiv/v3/288/all/graph.json\")\n",
    "# G = load_graph(\"out/experiments/finetune/arxiv/v3/288/test/graph.json\")\n",
    "exp = query(exp=\"finetune\", dataset=\"arxiv/v2\", reweighted=True)\n",
    "hp = PostProcessHP(**exp.best_hp(\"edge_soft_f1\"))\n",
    "G = post_process(G, hp)\n",
    "G = nx.subgraph(G, nx.descendants(G, G.graph[\"root\"]) | {G.graph[\"root\"]})\n",
    "print(hp)\n",
    "print(G.number_of_nodes(), G.number_of_edges())\n",
    "\n",
    "# A = display_graph(G, layout=\"dot\")\n",
    "# A = display_graph(G, layout=\"sfdp\", Glevels=2, GK=0.8, Gstart=2)\n",
    "# A = display_graph(G, layout=\"neato\", Elen=2, Gstart=0)\n",
    "A = display_graph(G, layout=\"twopi\", Granksep=2, Gstart=0)\n",
    "# A.draw(fig_dir / f\"{exp.name}_{exp.dataset.replace('/', '_')}_output.pdf\")\n",
    "# print(A.to_string())\n",
    "A\n",
    "\n",
    "# relabel_map = {}\n",
    "# for n, data in G.nodes(data=True):\n",
    "#     relabel_map[n] = data.get(\"title\", n)\n",
    "# G = nx.relabel_nodes(G, relabel_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "exp = query(exp=\"finetune\", dataset=\"arxiv/v2\", reweighted=True)\n",
    "G = load_graph(exp.train_input)\n",
    "print(G.number_of_nodes(), G.number_of_edges())\n",
    "\n",
    "# A = display_graph(G, layout=\"dot\")\n",
    "# A = display_graph(G, layout=\"sfdp\", Glevels=1, GK=0.6, Gstart=2)\n",
    "# A = display_graph(G, layout=\"neato\", Elen=1.5, Gstart=0)\n",
    "# A = display_graph(G, layout=\"twopi\", Granksep=2, Gstart=2, Groot='\"Main topic classifications\"')\n",
    "# A = display_graph(G, layout=\"circo\", Goverlap=\"compress\")\n",
    "# A.draw(fig_dir / f\"{exp.name}_test_output.pdf\")\n",
    "# print(A.to_string())\n",
    "# A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "G = load_graph(\"out/data/arxiv/v2/full/full_graph.json\")\n",
    "print(G.number_of_nodes(), G.number_of_edges())\n",
    "\n",
    "# A = display_graph(G, layout=\"dot\")\n",
    "# A = display_graph(G, layout=\"sfdp\", Glevels=1, GK=0.6, Gstart=2)\n",
    "# A = display_graph(G, layout=\"neato\", Elen=1.5, Gstart=0)\n",
    "A = display_graph(G, layout=\"twopi\", Granksep=3, Gstart=2)\n",
    "# A = display_graph(G, layout=\"circo\", Goverlap=\"compress\")\n",
    "A.draw(fig_dir / f\"arxiv_ground_truth.pdf\")\n",
    "# print(A.to_string())\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the union and intersection of nodes & edges in the train, eval and test graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"wikipedia/v2\": \"Wikipedia\",\n",
    "    \"arxiv/v2\": \"arXiv\",\n",
    "}\n",
    "\n",
    "for dataset, name in datasets.items():\n",
    "    fig, ax = plt.subplots(figsize=(4, 3))\n",
    "    exp = query(exp=\"hearst\", dataset=dataset)\n",
    "    G_train = load_graph(exp.train_input)\n",
    "    G_eval = load_graph(exp.eval_ground_truth)\n",
    "    G_test = load_graph(exp.test_ground_truth)\n",
    "\n",
    "    train = set(G_train.nodes())\n",
    "    eval = set(G_eval.nodes())\n",
    "    test = set(G_test.nodes())\n",
    "\n",
    "    matplotlib_venn.venn3([train, eval, test], [\"Train\", \"Eval\", \"Test\"], ax=ax)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(fig_dir / f\"{name}_train_eval_test_split.pdf\")\n",
    "\n",
    "\n",
    "# fig.savefig(fig_dir / \"wiki_train_eval_test_split.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the nodes & edges coverage by only considering paths of length n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_and_edge_coverage(G: nx.Graph, n: int):\n",
    "    G_gt, nx_to_gt_map, gt_to_nx_map = nx_to_gt(G)\n",
    "\n",
    "    nodes_with_pages = {\n",
    "        node for node, data in G.nodes(data=True) if len(data[\"pages\"]) > 0\n",
    "    }\n",
    "\n",
    "    nodes_covered = set()\n",
    "    edges_covered = set()\n",
    "    for node in tqdm(nodes_with_pages):\n",
    "        for path in gt.all_paths(\n",
    "            G_gt,\n",
    "            source=nx_to_gt_map[G.graph[\"root\"]],\n",
    "            target=nx_to_gt_map[node],\n",
    "            cutoff=n,\n",
    "        ):\n",
    "            edges_covered |= {\n",
    "                (gt_to_nx_map[u], gt_to_nx_map[v]) for u, v in zip(path[:-1], path[1:])\n",
    "            }\n",
    "            nodes_covered |= {gt_to_nx_map[v] for v in path}\n",
    "\n",
    "    assert nodes_covered.issubset(set(G.nodes()))\n",
    "    assert edges_covered.issubset(set(G.edges()))\n",
    "    return (\n",
    "        len(nodes_covered) / G.number_of_nodes(),\n",
    "        len(edges_covered) / G.number_of_edges(),\n",
    "    )\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 3))\n",
    "xs = np.arange(6)\n",
    "ys = np.array([node_and_edge_coverage(G_train, n) for n in xs])\n",
    "ax.plot(xs, ys[:, 0], label=\"Nodes coverage\")\n",
    "ax.plot(xs, ys[:, 1], label=\"Edges coverage\")\n",
    "fig.legend(loc=\"upper left\")\n",
    "\n",
    "# fig.savefig(fig_dir / \"wiki_test_coverage.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motif analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def regular_polygon_layout(g: gt.Graph, n: int, r: float = 1.0):\n",
    "    pos = g.new_vertex_property(\"vector<double>\")\n",
    "    for i, v in enumerate(g.vertices()):\n",
    "        pos[v] = (\n",
    "            r * np.cos(2 * np.pi * i / n - np.pi / 2),\n",
    "            r * np.sin(2 * np.pi * i / n - np.pi / 2),\n",
    "        )\n",
    "    return pos\n",
    "\n",
    "\n",
    "def count_motifs(Gs: list[nx.Graph], n: int = 3):\n",
    "    motifs_list, counts_list = zip(*[gt.motifs(nx_to_gt(G)[0], n) for G in Gs])\n",
    "    all_motifs = []\n",
    "    all_idx_to_idx = []\n",
    "    for motifs in motifs_list:\n",
    "        idx_to_idx = {}  # idx in all_motifs -> idx in motifs\n",
    "        for i, motif in enumerate(motifs):\n",
    "            for j, existing_motif in enumerate(all_motifs):\n",
    "                if gt.isomorphism(motif, existing_motif):\n",
    "                    idx_to_idx[j] = i\n",
    "                    break\n",
    "            else:\n",
    "                all_motifs.append(motif)\n",
    "                idx_to_idx[len(all_motifs) - 1] = i\n",
    "        all_idx_to_idx.append(idx_to_idx)\n",
    "\n",
    "    all_counts = []\n",
    "    for idx_to_idx, counts in zip(all_idx_to_idx, counts_list):\n",
    "        all_counts.append([])\n",
    "        for j in range(len(all_motifs)):\n",
    "            all_counts[-1].append(counts[idx_to_idx[j]] if j in idx_to_idx else 0)\n",
    "\n",
    "    return all_motifs, all_counts\n",
    "\n",
    "\n",
    "exps = [\n",
    "    query(exp=\"prompting\", k_shot=0, dataset=\"arxiv/v2\"),\n",
    "    query(exp=\"finetune\", reweighted=True, dataset=\"arxiv/v2\"),\n",
    "    query(exp=\"finetune\", reweighted=False, dataset=\"arxiv/v2\"),\n",
    "    # query(exp=\"finetune\", reweighted=True, dataset=\"wikipedia/v2\", step=\"final\"),\n",
    "    # query(exp=\"finetune\", reweighted=False, dataset=\"wikipedia/v2\", step=\"final\"),\n",
    "    # query(exp=\"prompting\", k_shot=3, dataset=\"wikipedia/v2\"),\n",
    "]\n",
    "assert all(exp.train_input == exps[0].train_input for exp in exps)\n",
    "assert all(exp.eval_ground_truth == exps[0].eval_ground_truth for exp in exps)\n",
    "assert all(exp.test_ground_truth == exps[0].test_ground_truth for exp in exps)\n",
    "\n",
    "labels = [exp.name for exp in exps] + [\"Ground truth\"]\n",
    "Gs = [\n",
    "    post_process(\n",
    "        load_graph(exp.test_output), PostProcessHP(**exp.best_hp(\"edge_similarity\"))\n",
    "    )\n",
    "    for exp in exps\n",
    "]\n",
    "Gs.append(load_graph(exps[0].test_ground_truth))\n",
    "\n",
    "motifs, counts = count_motifs(Gs)\n",
    "print(counts)\n",
    "\n",
    "# sort by the sum of counts\n",
    "counts = np.array(counts)\n",
    "order = np.argsort(counts.sum(axis=0))[::-1]\n",
    "counts = counts[:, order]\n",
    "motifs = [motifs[i] for i in order]\n",
    "\n",
    "# only keep the top n motifs\n",
    "# n = 10\n",
    "# counts = counts[:, :n]\n",
    "# motifs = motifs[:n]\n",
    "\n",
    "# normalize the counts\n",
    "counts = counts / counts.sum(axis=1)[:, None]\n",
    "\n",
    "df_test = pd.DataFrame(\n",
    "    {\n",
    "        \"count\": counts.reshape(-1),\n",
    "        \"motif\": np.tile(np.arange(len(motifs)), counts.shape[0]),\n",
    "        \"graph\": np.repeat(labels, len(motifs)),\n",
    "    }\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "sns.barplot(data=df_test, x=\"motif\", y=\"count\", hue=\"graph\", ax=ax)\n",
    "ax.set(xticklabels=[], xlabel=\"\", ylabel=\"Fraction of motifs\")\n",
    "\n",
    "# draw the motif graph as the x labels\n",
    "res = 5\n",
    "r = 0.7\n",
    "pad = 0.7\n",
    "for motif, xticklabel in zip(motifs, ax.get_xticklabels()):\n",
    "    gt.graph_draw(\n",
    "        motif,\n",
    "        pos=regular_polygon_layout(motif, motif.num_vertices(), r),\n",
    "        vertex_fill_color=\"black\",\n",
    "        # vertex_size=5,\n",
    "        edge_color=\"black\",\n",
    "        output_size=(30 * res, 30 * res),\n",
    "        output=\"/tmp/motif.png\",\n",
    "        ink_scale=0.6,\n",
    "        fit_view=(-r - pad, -r - pad, 2 * (r + pad), 2 * (r + pad)),\n",
    "    )\n",
    "    im = plt.imread(\"/tmp/motif.png\")\n",
    "    ib = OffsetImage(im, zoom=1 / res, snap=True, resample=True)\n",
    "    ib.image.axes = ax\n",
    "    ab = AnnotationBbox(\n",
    "        ib,\n",
    "        xticklabel.get_position(),\n",
    "        frameon=False,\n",
    "        box_alignment=(0.5, 1.1),\n",
    "    )\n",
    "    ax.add_artist(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=8)\n",
    "counts_all = np.zeros((len(labels), 54))\n",
    "counts_all[:, : counts.shape[1]] = counts\n",
    "counts_all += 1\n",
    "counts_all /= counts_all.sum(axis=1)[:, None]\n",
    "counts_true = counts_all[-1]\n",
    "\n",
    "kl = np.sum(counts_true * np.log(counts_true / counts_all), axis=1)\n",
    "kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AP vs training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = query_multiple(exp=\"finetune\", version=2)\n",
    "\n",
    "assert len({exp.eval_ground_truth for exp in exps}) == 1\n",
    "G_true = load_graph(exps[0].eval_ground_truth)\n",
    "\n",
    "\n",
    "def prec_recall_curve(thresholds, G_pred, G_true):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for edge_percentile in tqdm(thresholds):\n",
    "        G_pruned = post_process(\n",
    "            G_pred,\n",
    "            PostProcessHP(\n",
    "                absolute_percentile=edge_percentile,\n",
    "                merge_nodes_by_lemma=False,\n",
    "                prune_unconnected_nodes=True,\n",
    "            ),\n",
    "        )\n",
    "        prec = edge_precision(G_pruned, G_true)\n",
    "        rec = edge_recall(G_pruned, G_true)\n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "\n",
    "    return np.array(precisions), np.array(recalls)\n",
    "\n",
    "\n",
    "data = []\n",
    "for exp in exps:\n",
    "    G = load_graph(exp.eval_output)\n",
    "    thresholds = 1 - np.geomspace(1 / G.number_of_edges(), 1, 11)\n",
    "    precisions, recalls = prec_recall_curve(thresholds, G, G_true)\n",
    "    data.append(\n",
    "        {\n",
    "            \"step\": exp.step,\n",
    "            \"reweighted\": exp.reweighted,\n",
    "            \"precisions\": precisions,\n",
    "            \"recalls\": recalls,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_ap(group):\n",
    "    ap = np.trapz(group[\"precisions\"], group[\"recalls\"])\n",
    "    return pd.Series({\"ap\": ap})\n",
    "\n",
    "\n",
    "df_test = pd.concat([pd.DataFrame(d) for d in data])\n",
    "df_test[\"f1\"] = (\n",
    "    2\n",
    "    * df_test[\"precisions\"]\n",
    "    * df_test[\"recalls\"]\n",
    "    / (df_test[\"precisions\"] + df_test[\"recalls\"])\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 3))\n",
    "\n",
    "sns.lineplot(\n",
    "    data=df_test, x=\"recalls\", y=\"precisions\", hue=\"step\", style=\"reweighted\", ax=axs[0]\n",
    ")\n",
    "df_ap = df_test.groupby([\"step\", \"reweighted\"]).apply(agg_ap)\n",
    "sns.lineplot(data=df_ap, x=\"step\", y=\"ap\", style=\"reweighted\", ax=axs[1])\n",
    "df_f1 = df_test.groupby([\"step\", \"reweighted\"]).agg({\"f1\": \"max\"})\n",
    "sns.lineplot(data=df_f1, x=\"step\", y=\"f1\", style=\"reweighted\", ax=axs[2])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = query_multiple(exp=\"finetune\", version=1)\n",
    "\n",
    "assert len({exp.eval_ground_truth for exp in exps}) == 1\n",
    "G_true = load_graph(exps[0].eval_ground_truth)\n",
    "\n",
    "n_levels = 4\n",
    "\n",
    "\n",
    "data = []\n",
    "for exp in exps:\n",
    "    G = load_graph(exp.eval_output)\n",
    "    for level in range(n_levels):\n",
    "        target_edges = [\n",
    "            (title(G_true, u), title(G_true, v))\n",
    "            for u, v in nth_level_edges(G_true, level)\n",
    "        ]\n",
    "        weights = [\n",
    "            G.edges[u, v][\"weight\"] if G.has_edge(u, v) else 0 for u, v in target_edges\n",
    "        ]\n",
    "        data.append(\n",
    "            {\n",
    "                \"step\": exp.step,\n",
    "                \"reweighted\": exp.reweighted,\n",
    "                \"level\": level,\n",
    "                \"weight\": weights,\n",
    "            }\n",
    "        )\n",
    "\n",
    "df_test = pd.concat([pd.DataFrame(d) for d in data])\n",
    "\n",
    "fig, axs = plt.subplots(1, n_levels, figsize=(4 * n_levels, 3))\n",
    "\n",
    "for level, ax in enumerate(axs):\n",
    "    sns.lineplot(\n",
    "        data=df_test[df_test[\"level\"] == level],\n",
    "        x=\"step\",\n",
    "        y=\"weight\",\n",
    "        hue=\"reweighted\",\n",
    "        ax=ax,\n",
    "        errorbar=(\"ci\", 90),\n",
    "    )\n",
    "    ax.set(title=f\"Mean weight of level {level} edges\")\n",
    "\n",
    "fig.tight_layout()\n",
    "# fig.savefig(fig_dir / \"finetune_detailed_edge_weights_change.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HP results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    \"wikipedia_v2\": \"Wikipedia\",\n",
    "    \"arxiv_v2\": \"arXiv\",\n",
    "}\n",
    "metrics = {\n",
    "    # \"motif_kl\": \"Motif KL\",\n",
    "    \"num_nodes\": \"Num nodes\",\n",
    "    \"num_edges\": \"Num edges\",\n",
    "    \"edge_f1\": \"Literal F1 ($\\\\uparrow$)\",\n",
    "    \"edge_hard_f1\": \"Fuzzy F1 ($\\\\uparrow$)\",\n",
    "    \"edge_soft_f1\": \"Continuous F1 ($\\\\uparrow$)\",\n",
    "    \"graph_soft_f1\": \"Graph F1 ($\\\\uparrow$)\",\n",
    "    \"motif_wass\": \"Motif dist. ($\\\\downarrow$)\",\n",
    "    # \"graph_hard_f1\": \"Graph hard F1\",\n",
    "}\n",
    "data = []\n",
    "for dataset, dataset_name in dataset.items():\n",
    "    with open(f\"out/eval/{dataset}/test_metrics.jsonl\") as f:\n",
    "        data += [{**json.loads(line), \"dataset\": dataset_name} for line in f]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df[[\"dataset\", \"name\"] + list(metrics.keys())]\n",
    "df = df.rename(columns=metrics).rename(columns={\"dataset\": \"Dataset\", \"name\": \"Method\"})\n",
    "\n",
    "# make dataset and name hierarchical index\n",
    "df = df.set_index([\"Dataset\", \"Method\"])\n",
    "# print(df.to_latex(float_format=\"%.3f\"))\n",
    "display(df)\n",
    "\n",
    "# plot for each dataset\n",
    "for dataset in df.index.get_level_values(\"Dataset\").unique():\n",
    "    fig, axs = sized_subplots(n_axes=len(metrics), n_cols=3, ax_size=(4, 2))\n",
    "    axs = axs.flatten()\n",
    "    for metric, ax in zip(metrics.values(), axs):\n",
    "        sns.barplot(\n",
    "            data=df.loc[dataset].sort_values(metric), x=metric, y=\"Method\", ax=ax\n",
    "        )\n",
    "        ax.set(ylabel=\"\")\n",
    "    fig.tight_layout()\n",
    "    # fig.savefig(fig_dir / f\"{dataset}_test_metrics.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(\"out/experiments/finetune/v10/final/eval/hp_search.jsonl\") as f:\n",
    "    # with open(\"out/experiments/finetune/arxiv/v3/288/eval/hp_search.jsonl\") as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        hp = item.pop(\"hp\")\n",
    "        data.append({**hp, **item})\n",
    "df = pd.DataFrame(data)\n",
    "df.sort_values(\"edge_soft_f1\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "exp_paths = [\n",
    "    # Path(\"out/experiments/hearst/svd/arxiv/eval\").glob(\"k_*/hp_search.jsonl\"),\n",
    "    Path(\"out/experiments/hearst/svd/wiki/eval\").glob(\"k_*/hp_search.jsonl\"),\n",
    "    # Path(\"out/experiments/rebel/svd/arxiv/eval\").glob(\"k_*/hp_search.jsonl\"),\n",
    "    # Path(\"out/experiments/rebel/svd/wiki/eval\").glob(\"k_*/hp_search.jsonl\"),\n",
    "]\n",
    "for exp_path in exp_paths:\n",
    "    data = []\n",
    "    for path in exp_path:\n",
    "        with open(path) as f:\n",
    "            k = int(path.parent.name.split(\"_\")[1])\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                hp = item.pop(\"hp\")\n",
    "                data.append({\"svd_k\": k, **hp, **item})\n",
    "    df = pd.DataFrame(data)\n",
    "    display(df.sort_values(\"edge_soft_f1\", ascending=False))  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "exp = query(exp=\"rebel\", dataset=\"arxiv/v2\")\n",
    "\n",
    "# 61 ** 2 * , 42 ** 2 * 0.786689\n",
    "# 0.954664 / np.log(61), 0.786689 / np.log(42)\n",
    "\n",
    "G = load_graph(\"out/experiments/rebel/svd/arxiv/eval/k_1000/graph.json\")\n",
    "eval_weights = np.array([G.edges[u, v][\"weight\"] for u, v in G.edges()])\n",
    "\n",
    "G = load_graph(\"out/experiments/rebel/svd/arxiv/test/k_1000/graph.json\")\n",
    "test_weights = np.array([G.edges[u, v][\"weight\"] for u, v in G.edges()])\n",
    "\n",
    "ax = sns.histplot(\n",
    "    eval_weights, bins=50, alpha=0.5, label=\"Eval\", log_scale=True, stat=\"density\"\n",
    ")\n",
    "ax = sns.histplot(\n",
    "    test_weights, bins=50, alpha=0.5, label=\"Test\", log_scale=True, stat=\"density\"\n",
    ")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed comparison for reweighting objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "exp_base = query(exp=\"finetune\", step=\"final\", version=1, reweighted=False)\n",
    "exp_reweighted = query(exp=\"finetune\", step=\"final\", version=3, reweighted=True)\n",
    "exp_memorised = query(exp=\"memorization\")\n",
    "assert exp_base.eval_ground_truth == exp_reweighted.eval_ground_truth\n",
    "assert exp_base.train_input == exp_reweighted.train_input\n",
    "\n",
    "G_train = load_graph(exp_base.train_input)\n",
    "G_true = load_graph(exp_base.test_ground_truth)\n",
    "G_base = load_graph(exp_base.test_output)\n",
    "G_reweighted = load_graph(exp_reweighted.test_output)\n",
    "G_memorised = load_graph(exp_memorised.test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "G_base_edges = {\n",
    "    (G_base.nodes[u][\"title\"], G_base.nodes[v][\"title\"]) for u, v in G_base.edges()\n",
    "}\n",
    "G_reweighted_edges = {\n",
    "    (G_reweighted.nodes[u][\"title\"], G_reweighted.nodes[v][\"title\"])\n",
    "    for u, v in G_reweighted.edges()\n",
    "}\n",
    "\n",
    "node_distances = nx.single_source_shortest_path_length(G_true, G_true.graph[\"root\"])\n",
    "true_edges_by_dist = defaultdict(set)\n",
    "for u, v in G_true.edges:\n",
    "    true_edges_by_dist[node_distances[u]].add(\n",
    "        (G_true.nodes[u][\"title\"], G_true.nodes[v][\"title\"])\n",
    "    )\n",
    "\n",
    "base_total_weight = sum(G_base.edges[u, v][\"weight\"] for u, v in G_base_edges)\n",
    "reweighted_total_weight = sum(\n",
    "    G_reweighted.edges[u, v][\"weight\"] for u, v in G_reweighted_edges\n",
    ")\n",
    "memorised_total_weight = sum(\n",
    "    G_memorised.edges[u, v][\"weight\"] for u, v in G_memorised.edges\n",
    ")\n",
    "\n",
    "data = []\n",
    "\n",
    "in_domain_edges = {\n",
    "    (G_true.nodes[u][\"title\"], G_true.nodes[v][\"title\"])\n",
    "    for u, v in G_true.edges() & G_train.edges()\n",
    "}\n",
    "out_of_domain_edges = {\n",
    "    (G_true.nodes[u][\"title\"], G_true.nodes[v][\"title\"])\n",
    "    for u, v in G_true.edges() - G_train.edges()\n",
    "}\n",
    "for d, edges in true_edges_by_dist.items():\n",
    "    for method, G, domain, domain_edges, total_weight in [\n",
    "        (\"base\", G_base, \"in\", in_domain_edges, base_total_weight),\n",
    "        (\"base\", G_base, \"out\", out_of_domain_edges, base_total_weight),\n",
    "        (\"reweighted\", G_reweighted, \"in\", in_domain_edges, reweighted_total_weight),\n",
    "        (\n",
    "            \"reweighted\",\n",
    "            G_reweighted,\n",
    "            \"out\",\n",
    "            out_of_domain_edges,\n",
    "            reweighted_total_weight,\n",
    "        ),\n",
    "        (\"memorised\", G_memorised, \"in\", in_domain_edges, memorised_total_weight),\n",
    "        (\"memorised\", G_memorised, \"out\", out_of_domain_edges, memorised_total_weight),\n",
    "    ]:\n",
    "        domain_edges = edges & domain_edges & G.edges()\n",
    "        for edge in domain_edges:\n",
    "            data.append(\n",
    "                {\n",
    "                    \"level\": d,\n",
    "                    \"method\": method,\n",
    "                    \"weight\": G.edges[edge][\"weight\"] / total_weight,\n",
    "                    \"domain\": domain,\n",
    "                }\n",
    "            )\n",
    "\n",
    "df_test = pd.DataFrame(data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "sns.lineplot(data=df_test, x=\"level\", y=\"weight\", hue=\"method\", style=\"domain\", ax=ax)\n",
    "ax.set(\n",
    "    yscale=\"log\",\n",
    "    xticks=np.arange(max(node_distances.values()) + 1),\n",
    "    xlabel=\"Distance from root\",\n",
    "    ylabel=\"Edge weights\",\n",
    ")\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "fig.tight_layout()\n",
    "# fig.savefig(fig_dir / \"finetune_reweighted_edge_weights.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-ol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
