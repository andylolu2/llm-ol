{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib_venn\n",
    "import graph_tool.all as gt\n",
    "from tqdm import tqdm\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "from llm_ol.dataset.data_model import load_graph\n",
    "from llm_ol.utils.nx_to_gt import nx_to_gt\n",
    "from llm_ol.experiments.post_processing import hp_search, post_process, PostProcessHP\n",
    "from llm_ol.eval.graph_metrics import edge_prec_recall_f1\n",
    "from metadata import query, query_multiple\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Some utilities\n",
    "\n",
    "fig_dir = Path(\"out\", \"graphs\")\n",
    "\n",
    "\n",
    "def display_graph(G: nx.Graph, layout: str = \"dot\", **kwargs):\n",
    "    relabel_map = {}\n",
    "    for n, data in G.nodes(data=True):\n",
    "        relabel_map[n] = data.get(\"title\", n)\n",
    "    G = nx.relabel_nodes(G, relabel_map)\n",
    "    A = nx.nx_agraph.to_agraph(G)\n",
    "    A.node_attr.update(fontname=\"Helvetica\", fontsize=10, shape=\"plaintext\")\n",
    "    A.graph_attr.update(ratio=\"compress\")\n",
    "    A.edge_attr.update(arrowsize=0.5)\n",
    "    for k, v in kwargs.items():\n",
    "        if k.startswith(\"G\"):\n",
    "            A.graph_attr[k[1:]] = v\n",
    "        elif k.startswith(\"N\"):\n",
    "            A.node_attr[k[1:]] = v\n",
    "        elif k.startswith(\"E\"):\n",
    "            A.edge_attr[k[1:]] = v\n",
    "    A.layout(layout)\n",
    "    return A\n",
    "\n",
    "\n",
    "def nth_level_nodes(G: nx.Graph, n: int):\n",
    "    return nx.descendants_at_distance(G, G.graph[\"root\"], n)\n",
    "\n",
    "\n",
    "def nth_level_edges(G: nx.Graph, n: int):\n",
    "    distances = nx.single_source_shortest_path_length(G, G.graph[\"root\"], cutoff=n)\n",
    "    return {(u, v) for u, v in G.edges() if distances.get(u, None) == n}\n",
    "\n",
    "\n",
    "def title(G, n):\n",
    "    return G.nodes[n].get(\"title\", n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot subgraph induced by paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "exp = query(exp=\"finetune\", reweighted=True, dataset=\"arxiv/v2\")\n",
    "G = load_graph(exp.eval_ground_truth)\n",
    "# G = post_process(\n",
    "#     G, PostProcessHP(absolute_percentile=0.975)\n",
    "# )\n",
    "A = display_graph(G, layout=\"fdp\", GK=0.15, Gsep=\"+1\")\n",
    "# A.draw(fig_dir / \"arxiv_eval_graph.pdf\")\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cutoff = 3\n",
    "node = random.choice(list(G.nodes()))\n",
    "\n",
    "G_sub = nx.DiGraph()\n",
    "for path in nx.all_simple_paths(G, G.graph[\"root\"], node, cutoff=path_cutoff):\n",
    "    for u, v in zip(path[:-1], path[1:]):\n",
    "        G_sub.add_edge(title(G, u), title(G, v))\n",
    "\n",
    "display_graph(G_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the union and intersection of nodes & edges in the train, eval and test graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = query(exp=\"hearst\")\n",
    "G_train = load_graph(exp.train_input)\n",
    "G_eval = load_graph(exp.eval_ground_truth)\n",
    "G_test = load_graph(exp.test_ground_truth)\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "for level, ax in enumerate(axs[0], start=1):\n",
    "    set1 = nth_level_nodes(G_train, level)\n",
    "    set2 = nth_level_nodes(G_eval, level)\n",
    "    set3 = nth_level_nodes(G_test, level)\n",
    "    matplotlib_venn.venn3([set1, set2, set3], [\"Train\", \"Eval\", \"Test\"], ax=ax)\n",
    "    ax.set_title(f\"Level {level} nodes\")\n",
    "\n",
    "for level, ax in enumerate(axs[1], start=0):\n",
    "    set1 = nth_level_edges(G_train, level)\n",
    "    set2 = nth_level_edges(G_eval, level)\n",
    "    set3 = nth_level_edges(G_test, level)\n",
    "    matplotlib_venn.venn3([set1, set2, set3], [\"Train\", \"Eval\", \"Test\"], ax=ax)\n",
    "    ax.set_title(f\"Level {level} edges\")\n",
    "\n",
    "# fig.savefig(fig_dir / \"wiki_train_eval_test_split.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the nodes & edges coverage by only considering paths of length n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_and_edge_coverage(G: nx.Graph, n: int):\n",
    "    G_gt, nx_to_gt_map, gt_to_nx_map = nx_to_gt(G)\n",
    "\n",
    "    nodes_with_pages = {\n",
    "        node for node, data in G.nodes(data=True) if len(data[\"pages\"]) > 0\n",
    "    }\n",
    "\n",
    "    nodes_covered = set()\n",
    "    edges_covered = set()\n",
    "    for node in tqdm(nodes_with_pages):\n",
    "        for path in gt.all_paths(\n",
    "            G_gt,\n",
    "            source=nx_to_gt_map[G.graph[\"root\"]],\n",
    "            target=nx_to_gt_map[node],\n",
    "            cutoff=n,\n",
    "        ):\n",
    "            edges_covered |= {\n",
    "                (gt_to_nx_map[u], gt_to_nx_map[v]) for u, v in zip(path[:-1], path[1:])\n",
    "            }\n",
    "            nodes_covered |= {gt_to_nx_map[v] for v in path}\n",
    "\n",
    "    assert nodes_covered.issubset(set(G.nodes()))\n",
    "    assert edges_covered.issubset(set(G.edges()))\n",
    "    return (\n",
    "        len(nodes_covered) / G.number_of_nodes(),\n",
    "        len(edges_covered) / G.number_of_edges(),\n",
    "    )\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 3))\n",
    "xs = np.arange(6)\n",
    "ys = np.array([node_and_edge_coverage(G_train, n) for n in xs])\n",
    "ax.plot(xs, ys[:, 0], label=\"Nodes coverage\")\n",
    "ax.plot(xs, ys[:, 1], label=\"Edges coverage\")\n",
    "fig.legend(loc=\"upper left\")\n",
    "\n",
    "# fig.savefig(fig_dir / \"wiki_test_coverage.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motif analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def regular_polygon_layout(g: gt.Graph, n: int, r: float = 1.0):\n",
    "    pos = g.new_vertex_property(\"vector<double>\")\n",
    "    for i, v in enumerate(g.vertices()):\n",
    "        pos[v] = (\n",
    "            r * np.cos(2 * np.pi * i / n - np.pi / 2),\n",
    "            r * np.sin(2 * np.pi * i / n - np.pi / 2),\n",
    "        )\n",
    "    return pos\n",
    "\n",
    "\n",
    "def count_motifs(Gs: list[nx.Graph], n: int = 3):\n",
    "    motifs_list, counts_list = zip(*[gt.motifs(nx_to_gt(G)[0], n) for G in Gs])\n",
    "    all_motifs = []\n",
    "    all_idx_to_idx = []\n",
    "    for motifs in motifs_list:\n",
    "        idx_to_idx = {}  # idx in all_motifs -> idx in motifs\n",
    "        for i, motif in enumerate(motifs):\n",
    "            for j, existing_motif in enumerate(all_motifs):\n",
    "                if gt.isomorphism(motif, existing_motif):\n",
    "                    idx_to_idx[j] = i\n",
    "                    break\n",
    "            else:\n",
    "                all_motifs.append(motif)\n",
    "                idx_to_idx[len(all_motifs) - 1] = i\n",
    "        all_idx_to_idx.append(idx_to_idx)\n",
    "\n",
    "    all_counts = []\n",
    "    for idx_to_idx, counts in zip(all_idx_to_idx, counts_list):\n",
    "        all_counts.append([])\n",
    "        for j in range(len(all_motifs)):\n",
    "            all_counts[-1].append(counts[idx_to_idx[j]] if j in idx_to_idx else 0)\n",
    "\n",
    "    return all_motifs, all_counts\n",
    "\n",
    "\n",
    "exps = [\n",
    "    query(exp=\"finetune\", reweighted=True, dataset=\"wikipedia/v2\", step=\"final\"),\n",
    "    # query(exp=\"finetune\", reweighted=False, dataset=\"wikipedia/v2\", step=\"final\"),\n",
    "    # query(exp=\"prompting\", k_shot=3, dataset=\"wikipedia/v2\"),\n",
    "]\n",
    "assert all(exp.train_input == exps[0].train_input for exp in exps)\n",
    "assert all(exp.eval_ground_truth == exps[0].eval_ground_truth for exp in exps)\n",
    "assert all(exp.test_ground_truth == exps[0].test_ground_truth for exp in exps)\n",
    "\n",
    "labels = [exp.name for exp in exps] + [\"Ground truth\"]\n",
    "Gs = [load_graph(exp.test_output) for exp in exps] + [\n",
    "    load_graph(exps[0].test_ground_truth)\n",
    "]\n",
    "\n",
    "motifs, counts = count_motifs(Gs)\n",
    "\n",
    "# sort by the sum of counts\n",
    "counts = np.array(counts)\n",
    "order = np.argsort(counts.sum(axis=0))[::-1]\n",
    "counts = counts[:, order]\n",
    "motifs = [motifs[i] for i in order]\n",
    "\n",
    "# only keep the top n motifs\n",
    "n = 5\n",
    "counts = counts[:, :n]\n",
    "motifs = motifs[:n]\n",
    "\n",
    "# normalize the counts\n",
    "counts = counts / counts.sum(axis=1)[:, None]\n",
    "\n",
    "df_test = pd.DataFrame(\n",
    "    {\n",
    "        \"count\": counts.reshape(-1),\n",
    "        \"motif\": np.tile(np.arange(len(motifs)), counts.shape[0]),\n",
    "        \"graph\": np.repeat(labels, len(motifs)),\n",
    "    }\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "sns.barplot(data=df_test, x=\"motif\", y=\"count\", hue=\"graph\", ax=ax)\n",
    "ax.set(xticklabels=[], xlabel=\"\", ylabel=\"Fraction of motifs\")\n",
    "\n",
    "# draw the motif graph as the x labels\n",
    "res = 5\n",
    "r = 0.7\n",
    "pad = 0.7\n",
    "for motif, xticklabel in zip(motifs, ax.get_xticklabels()):\n",
    "    gt.graph_draw(\n",
    "        motif,\n",
    "        pos=regular_polygon_layout(motif, motif.num_vertices(), r),\n",
    "        vertex_fill_color=\"black\",\n",
    "        # vertex_size=5,\n",
    "        edge_color=\"black\",\n",
    "        output_size=(30 * res, 30 * res),\n",
    "        output=\"/tmp/motif.png\",\n",
    "        ink_scale=0.6,\n",
    "        fit_view=(-r - pad, -r - pad, 2 * (r + pad), 2 * (r + pad)),\n",
    "    )\n",
    "    im = plt.imread(\"/tmp/motif.png\")\n",
    "    ib = OffsetImage(im, zoom=1 / res, snap=True, resample=True)\n",
    "    ib.image.axes = ax\n",
    "    ab = AnnotationBbox(\n",
    "        ib,\n",
    "        xticklabel.get_position(),\n",
    "        frameon=False,\n",
    "        box_alignment=(0.5, 1.1),\n",
    "    )\n",
    "    ax.add_artist(ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AP vs training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = query_multiple(exp=\"finetune\", version=2)\n",
    "\n",
    "assert len({exp.eval_ground_truth for exp in exps}) == 1\n",
    "G_true = load_graph(exps[0].eval_ground_truth)\n",
    "\n",
    "\n",
    "def prec_recall_curve(thresholds, G_pred, G_true):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for edge_percentile in tqdm(thresholds):\n",
    "        G_pruned = post_process(\n",
    "            G_pred,\n",
    "            PostProcessHP(\n",
    "                absolute_percentile=edge_percentile,\n",
    "                merge_nodes_by_lemma=False,\n",
    "                prune_unconnected_nodes=True,\n",
    "            ),\n",
    "        )\n",
    "        prec = edge_precision(G_pruned, G_true)\n",
    "        rec = edge_recall(G_pruned, G_true)\n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "\n",
    "    return np.array(precisions), np.array(recalls)\n",
    "\n",
    "\n",
    "data = []\n",
    "for exp in exps:\n",
    "    G = load_graph(exp.eval_output)\n",
    "    thresholds = 1 - np.geomspace(1 / G.number_of_edges(), 1, 11)\n",
    "    precisions, recalls = prec_recall_curve(thresholds, G, G_true)\n",
    "    data.append(\n",
    "        {\n",
    "            \"step\": exp.step,\n",
    "            \"reweighted\": exp.reweighted,\n",
    "            \"precisions\": precisions,\n",
    "            \"recalls\": recalls,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_ap(group):\n",
    "    ap = np.trapz(group[\"precisions\"], group[\"recalls\"])\n",
    "    return pd.Series({\"ap\": ap})\n",
    "\n",
    "\n",
    "df_test = pd.concat([pd.DataFrame(d) for d in data])\n",
    "df_test[\"f1\"] = (\n",
    "    2\n",
    "    * df_test[\"precisions\"]\n",
    "    * df_test[\"recalls\"]\n",
    "    / (df_test[\"precisions\"] + df_test[\"recalls\"])\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 3))\n",
    "\n",
    "sns.lineplot(\n",
    "    data=df_test, x=\"recalls\", y=\"precisions\", hue=\"step\", style=\"reweighted\", ax=axs[0]\n",
    ")\n",
    "df_ap = df_test.groupby([\"step\", \"reweighted\"]).apply(agg_ap)\n",
    "sns.lineplot(data=df_ap, x=\"step\", y=\"ap\", style=\"reweighted\", ax=axs[1])\n",
    "df_f1 = df_test.groupby([\"step\", \"reweighted\"]).agg({\"f1\": \"max\"})\n",
    "sns.lineplot(data=df_f1, x=\"step\", y=\"f1\", style=\"reweighted\", ax=axs[2])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = query_multiple(exp=\"finetune\", version=1)\n",
    "\n",
    "assert len({exp.eval_ground_truth for exp in exps}) == 1\n",
    "G_true = load_graph(exps[0].eval_ground_truth)\n",
    "\n",
    "n_levels = 4\n",
    "\n",
    "\n",
    "data = []\n",
    "for exp in exps:\n",
    "    G = load_graph(exp.eval_output)\n",
    "    for level in range(n_levels):\n",
    "        target_edges = [\n",
    "            (title(G_true, u), title(G_true, v))\n",
    "            for u, v in nth_level_edges(G_true, level)\n",
    "        ]\n",
    "        weights = [\n",
    "            G.edges[u, v][\"weight\"] if G.has_edge(u, v) else 0 for u, v in target_edges\n",
    "        ]\n",
    "        data.append(\n",
    "            {\n",
    "                \"step\": exp.step,\n",
    "                \"reweighted\": exp.reweighted,\n",
    "                \"level\": level,\n",
    "                \"weight\": weights,\n",
    "            }\n",
    "        )\n",
    "\n",
    "df_test = pd.concat([pd.DataFrame(d) for d in data])\n",
    "\n",
    "fig, axs = plt.subplots(1, n_levels, figsize=(4 * n_levels, 3))\n",
    "\n",
    "for level, ax in enumerate(axs):\n",
    "    sns.lineplot(\n",
    "        data=df_test[df_test[\"level\"] == level],\n",
    "        x=\"step\",\n",
    "        y=\"weight\",\n",
    "        hue=\"reweighted\",\n",
    "        ax=ax,\n",
    "        errorbar=(\"ci\", 90),\n",
    "    )\n",
    "    ax.set(title=f\"Mean weight of level {level} edges\")\n",
    "\n",
    "fig.tight_layout()\n",
    "# fig.savefig(fig_dir / \"finetune_detailed_edge_weights_change.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HP resutls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "exps = [\n",
    "    query(exp=\"prompting\", k_shot=0),\n",
    "    query(exp=\"prompting\", k_shot=1),\n",
    "    query(exp=\"prompting\", k_shot=3),\n",
    "    query(exp=\"memorization\"),\n",
    "    query(exp=\"hearst\"),\n",
    "    query(exp=\"finetune\", step=\"final\", reweighted=False),\n",
    "    query(exp=\"finetune\", step=\"final\", reweighted=True, version=4),\n",
    "    # query(exp=\"finetune\", step=10000, version=3),\n",
    "    # query(exp=\"finetune\", step=15000, version=3),\n",
    "    # query(exp=\"finetune\", step=16500, version=1, reweighted=False),\n",
    "]\n",
    "\n",
    "metric = \"edge_f1\"\n",
    "metrics = []\n",
    "prec_recall = []\n",
    "\n",
    "for exp in exps:\n",
    "    result = exp.eval_hp_result\n",
    "    with open(result, \"r\") as f:\n",
    "        data = []\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            hp = item.pop(\"hp\")\n",
    "            data.append({**hp, **item})\n",
    "        df_eval = pd.DataFrame(data)\n",
    "    best_row = df_eval.iloc[df_eval[metric].idxmax()]\n",
    "\n",
    "    result = exp.test_hp_result\n",
    "    with open(result, \"r\") as f:\n",
    "        data = []\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            hp = item.pop(\"hp\")\n",
    "            data.append({**hp, **item})\n",
    "        df_test = pd.DataFrame(data)\n",
    "\n",
    "    df_test[\"distance\"] = np.abs(\n",
    "        df_test[\"relative_percentile\"] - best_row[\"relative_percentile\"]\n",
    "    ) + np.abs(df_test[\"graph_similarity\"] - best_row[\"graph_similarity\"])\n",
    "    best_row_test = df_test.iloc[df_test[\"distance\"].idxmin()]\n",
    "    best_f1 = best_row_test[\"edge_f1\"]\n",
    "    best_fuzzy_f1 = best_row_test[\"fuzzy_edge_f1\"]\n",
    "    best_sim = best_row_test[\"graph_similarity\"]\n",
    "    best_edge_sim = best_row_test[\"edge_similarity\"]\n",
    "\n",
    "    df_sub = df_test.query(\"relative_percentile == 1\")\n",
    "    prec = df_sub[\"fuzzy_edge_precision\"].tolist()\n",
    "    rec = df_sub[\"fuzzy_edge_recall\"].tolist()\n",
    "    prec, rec = np.array(prec), np.array(rec)\n",
    "    order = np.argsort(rec)\n",
    "    prec, rec = prec[order], rec[order]\n",
    "\n",
    "    ap = np.trapz(x=np.append(rec, 1), y=np.append(prec, 0))\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"name\": exp.name,\n",
    "            \"ap\": ap,\n",
    "            \"best_f1\": best_f1,\n",
    "            \"best_sim\": best_sim,\n",
    "            \"best_fuzzy_f1\": best_fuzzy_f1,\n",
    "            \"best_edge_sim\": best_edge_sim,\n",
    "        }\n",
    "    )\n",
    "    prec_recall.append(pd.DataFrame({\"prec\": prec, \"rec\": rec, \"name\": exp.name}))\n",
    "\n",
    "prec_recall = pd.concat(prec_recall)\n",
    "metrics = pd.DataFrame(metrics)\n",
    "display(metrics)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=4, figsize=(20, 6))\n",
    "\n",
    "ax = sns.lineplot(data=prec_recall, x=\"rec\", y=\"prec\", hue=\"name\", ax=axs[0])\n",
    "ax.set(xlabel=\"Recall\", ylabel=\"Precision\")  # , xscale=\"log\", yscale=\"log\")\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# ax = sns.barplot(\n",
    "#     data=metrics.sort_values(\"best_sim\"), x=\"name\", y=\"best_sim\", ax=axs[1]\n",
    "# )\n",
    "# ax.set(ylim=(0.7, 0.9))\n",
    "\n",
    "ax = sns.barplot(\n",
    "    data=metrics.sort_values(\"best_fuzzy_f1\"), x=\"name\", y=\"best_fuzzy_f1\", ax=axs[1]\n",
    ")\n",
    "\n",
    "ax = sns.barplot(data=metrics.sort_values(\"best_f1\"), x=\"name\", y=\"best_f1\", ax=axs[2])\n",
    "\n",
    "ax = sns.barplot(\n",
    "    data=metrics.sort_values(\"best_edge_sim\"), x=\"name\", y=\"best_edge_sim\", ax=axs[3]\n",
    ")\n",
    "\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_dir / \"hp_search_prec_recall_all.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed comparison for reweighting objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "exp_base = query(exp=\"finetune\", step=\"final\", version=1, reweighted=False)\n",
    "exp_reweighted = query(exp=\"finetune\", step=\"final\", version=3, reweighted=True)\n",
    "exp_memorised = query(exp=\"memorization\")\n",
    "assert exp_base.eval_ground_truth == exp_reweighted.eval_ground_truth\n",
    "assert exp_base.train_input == exp_reweighted.train_input\n",
    "\n",
    "G_train = load_graph(exp_base.train_input)\n",
    "G_true = load_graph(exp_base.test_ground_truth)\n",
    "G_base = load_graph(exp_base.test_output)\n",
    "G_reweighted = load_graph(exp_reweighted.test_output)\n",
    "G_memorised = load_graph(exp_memorised.test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "G_base_edges = {\n",
    "    (G_base.nodes[u][\"title\"], G_base.nodes[v][\"title\"]) for u, v in G_base.edges()\n",
    "}\n",
    "G_reweighted_edges = {\n",
    "    (G_reweighted.nodes[u][\"title\"], G_reweighted.nodes[v][\"title\"])\n",
    "    for u, v in G_reweighted.edges()\n",
    "}\n",
    "\n",
    "node_distances = nx.single_source_shortest_path_length(G_true, G_true.graph[\"root\"])\n",
    "true_edges_by_dist = defaultdict(set)\n",
    "for u, v in G_true.edges:\n",
    "    true_edges_by_dist[node_distances[u]].add(\n",
    "        (G_true.nodes[u][\"title\"], G_true.nodes[v][\"title\"])\n",
    "    )\n",
    "\n",
    "base_total_weight = sum(G_base.edges[u, v][\"weight\"] for u, v in G_base_edges)\n",
    "reweighted_total_weight = sum(\n",
    "    G_reweighted.edges[u, v][\"weight\"] for u, v in G_reweighted_edges\n",
    ")\n",
    "memorised_total_weight = sum(\n",
    "    G_memorised.edges[u, v][\"weight\"] for u, v in G_memorised.edges\n",
    ")\n",
    "\n",
    "data = []\n",
    "\n",
    "in_domain_edges = {\n",
    "    (G_true.nodes[u][\"title\"], G_true.nodes[v][\"title\"])\n",
    "    for u, v in G_true.edges() & G_train.edges()\n",
    "}\n",
    "out_of_domain_edges = {\n",
    "    (G_true.nodes[u][\"title\"], G_true.nodes[v][\"title\"])\n",
    "    for u, v in G_true.edges() - G_train.edges()\n",
    "}\n",
    "for d, edges in true_edges_by_dist.items():\n",
    "    for method, G, domain, domain_edges, total_weight in [\n",
    "        (\"base\", G_base, \"in\", in_domain_edges, base_total_weight),\n",
    "        (\"base\", G_base, \"out\", out_of_domain_edges, base_total_weight),\n",
    "        (\"reweighted\", G_reweighted, \"in\", in_domain_edges, reweighted_total_weight),\n",
    "        (\n",
    "            \"reweighted\",\n",
    "            G_reweighted,\n",
    "            \"out\",\n",
    "            out_of_domain_edges,\n",
    "            reweighted_total_weight,\n",
    "        ),\n",
    "        (\"memorised\", G_memorised, \"in\", in_domain_edges, memorised_total_weight),\n",
    "        (\"memorised\", G_memorised, \"out\", out_of_domain_edges, memorised_total_weight),\n",
    "    ]:\n",
    "        domain_edges = edges & domain_edges & G.edges()\n",
    "        for edge in domain_edges:\n",
    "            data.append(\n",
    "                {\n",
    "                    \"level\": d,\n",
    "                    \"method\": method,\n",
    "                    \"weight\": G.edges[edge][\"weight\"] / total_weight,\n",
    "                    \"domain\": domain,\n",
    "                }\n",
    "            )\n",
    "\n",
    "df_test = pd.DataFrame(data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "sns.lineplot(data=df_test, x=\"level\", y=\"weight\", hue=\"method\", style=\"domain\", ax=ax)\n",
    "ax.set(\n",
    "    yscale=\"log\",\n",
    "    xticks=np.arange(max(node_distances.values()) + 1),\n",
    "    xlabel=\"Distance from root\",\n",
    "    ylabel=\"Edge weights\",\n",
    ")\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "fig.tight_layout()\n",
    "# fig.savefig(fig_dir / \"finetune_reweighted_edge_weights.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-ol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
