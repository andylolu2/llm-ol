{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%env OMP_NUM_THREADS=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import dotenv\n",
    "from transformers import pipeline\n",
    "\n",
    "from llm_ol.dataset import data_model\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "triplet_extractor = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"Babelscape/rebel-large\",\n",
    "    tokenizer=\"Babelscape/rebel-large\",\n",
    ")\n",
    "\n",
    "\n",
    "# Function to parse the generated text and extract the triplets\n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = \"\", \"\", \"\", \"\"\n",
    "    text = text.strip()\n",
    "    current = \"x\"\n",
    "    for token in (\n",
    "        text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split()\n",
    "    ):\n",
    "        if token == \"<triplet>\":\n",
    "            current = \"t\"\n",
    "            if relation != \"\":\n",
    "                triplets.append(\n",
    "                    {\n",
    "                        \"head\": subject.strip(),\n",
    "                        \"type\": relation.strip(),\n",
    "                        \"tail\": object_.strip(),\n",
    "                    }\n",
    "                )\n",
    "                relation = \"\"\n",
    "            subject = \"\"\n",
    "        elif token == \"<subj>\":\n",
    "            current = \"s\"\n",
    "            if relation != \"\":\n",
    "                triplets.append(\n",
    "                    {\n",
    "                        \"head\": subject.strip(),\n",
    "                        \"type\": relation.strip(),\n",
    "                        \"tail\": object_.strip(),\n",
    "                    }\n",
    "                )\n",
    "            object_ = \"\"\n",
    "        elif token == \"<obj>\":\n",
    "            current = \"o\"\n",
    "            relation = \"\"\n",
    "        else:\n",
    "            if current == \"t\":\n",
    "                subject += \" \" + token\n",
    "            elif current == \"s\":\n",
    "                object_ += \" \" + token\n",
    "            elif current == \"o\":\n",
    "                relation += \" \" + token\n",
    "    if subject != \"\" and relation != \"\" and object_ != \"\":\n",
    "        triplets.append(\n",
    "            {\"head\": subject.strip(), \"type\": relation.strip(), \"tail\": object_.strip()}\n",
    "        )\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "G = data_model.load_graph(\"out/data/wikipedia/v2/train_eval_split/train_graph.json\")\n",
    "\n",
    "pages = [page for n in G.nodes for page in G.nodes[n][\"pages\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "abstract = random.choice(pages)[\"abstract\"]\n",
    "\n",
    "# We need to use the tokenizer manually since we need special tokens.\n",
    "extracted_text = triplet_extractor.tokenizer.batch_decode(\n",
    "    [\n",
    "        triplet_extractor(abstract, return_tensors=True, return_text=False)[0][\n",
    "            \"generated_token_ids\"\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(abstract)\n",
    "print(extracted_text[0])\n",
    "extracted_triplets = extract_triplets(extracted_text[0])\n",
    "print(extracted_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "abstract1 = random.choice(pages)[\"abstract\"]\n",
    "abstract2 = random.choice(pages)[\"abstract\"]\n",
    "\n",
    "triplet_extractor([abstract1, abstract2])  # , return_tensors=True, return_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "counts = defaultdict(set)\n",
    "with open(\"out/experiments/rebel/v1/test/categorised_pages.jsonl\") as f:\n",
    "    for line in f:\n",
    "        page = json.loads(line)\n",
    "        for triplet in page[\"triplets\"]:\n",
    "            counts[triplet[\"type\"]].add((triplet[\"tail\"], triplet[\"head\"]))\n",
    "\n",
    "print(\n",
    "    sorted([(k, len(v)) for k, v in counts.items()], key=lambda x: x[1], reverse=True)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-ol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
