{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import dotenv\n",
    "from accelerate import PartialState\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import DataCollatorForCompletionOnlyLM, SFTTrainer\n",
    "from absl import logging\n",
    "\n",
    "from llm_ol.experiments.llm.finetune.training.utils import GenerateSamplesCallback\n",
    "from llm_ol.experiments.llm.templates import (\n",
    "    _MISTRAL_TEMPLATE,\n",
    "    PROMPT_TEMPLATE,\n",
    "    RESPONSE_TEMPLATE,\n",
    ")\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\n",
    "def dataset_from_file(\n",
    "    data_file: str | Path, size: int | None = None, seed: int = 0\n",
    ") -> Dataset:\n",
    "    dataset = load_dataset(\"json\", data_files=str(data_file), split=\"train\")\n",
    "    assert isinstance(dataset, Dataset)\n",
    "    if size is not None:\n",
    "        dataset = dataset.shuffle(seed=seed).select(range(size))\n",
    "\n",
    "    def make_messages(examples: dict[str, list]) -> dict[str, list]:\n",
    "        outputs = []\n",
    "        for title, abstract, paths in zip(\n",
    "            examples[\"title\"], examples[\"abstract\"], examples[\"paths\"]\n",
    "        ):\n",
    "            prompt = PROMPT_TEMPLATE.render(title=title, abstract=abstract)\n",
    "            response = RESPONSE_TEMPLATE.render(paths=paths)\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                {\"role\": \"assistant\", \"content\": response},\n",
    "            ]\n",
    "            outputs.append(messages)\n",
    "        return {\"messages\": outputs}\n",
    "\n",
    "    dataset = dataset.map(make_messages, batched=True, num_proc=16)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"out/experiments/finetune/v9/train/checkpoint-15000/merged\"\n",
    "device_string = PartialState().process_index\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    use_cache=False,\n",
    "    device_map={\"\": device_string},\n",
    "    torch_dtype=\"auto\",\n",
    ")\n",
    "model = get_peft_model(\n",
    "    model,\n",
    "    LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    ),\n",
    ")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.chat_template = _MISTRAL_TEMPLATE\n",
    "tokenizer.padding_side = \"right\"\n",
    "if getattr(tokenizer, \"pad_token\", None) is None:\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=[733, 28748, 16289, 28793],\n",
    "    instruction_template=[733, 16289, 28793],\n",
    "    tokenizer=tokenizer,\n",
    "    pad_to_multiple_of=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    max_seq_length=2048,\n",
    "    dataset_num_proc=16,\n",
    "    train_dataset=dataset_from_file(\n",
    "        \"out/experiments/llm/arxiv/train_dataset.jsonl\", 1024\n",
    "    ),\n",
    "    eval_dataset=dataset_from_file(\"out/experiments/llm/arxiv/eval_dataset.jsonl\", 128),\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,\n",
    "    },\n",
    "    callbacks=[GenerateSamplesCallback(5, [733, 28748, 16289, 28793])],\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"/tmp/llm\",\n",
    "        overwrite_output_dir=True,\n",
    "        optim=\"adamw_torch_fused\",\n",
    "        learning_rate=1e-5,\n",
    "        lr_scheduler_type=\"constant_with_warmup\",\n",
    "        warmup_steps=0,\n",
    "        report_to=[],\n",
    "        num_train_epochs=2,\n",
    "        logging_steps=10,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        gradient_accumulation_steps=1,\n",
    "        ddp_find_unused_parameters=False,\n",
    "        group_by_length=True,\n",
    "        fp16=torch.cuda.is_available() and not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(),\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=20,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        seed=0,\n",
    "        data_seed=0,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-ol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
