{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import dotenv\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from llm_ol.experiments.llm.templates import (\n",
    "    MISTRAL_TEMPLATE,\n",
    "    PROMPT_TEMPLATE,\n",
    "    RESPONSE_TEMPLATE,\n",
    ")\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"alpindale/Mistral-7B-v0.2-hf\", add_prefix_space=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"TITLE 123\"\n",
    "abstract = \"This is an abstract. It is a very good abstract. It is the best abstract.\"\n",
    "\n",
    "paths = [\n",
    "    [\"Hello\", \"baked apples\", \"world!\"],\n",
    "    [\"Earth sciences\", \"Geology\", \"Geophysics\"],\n",
    "]\n",
    "\n",
    "\n",
    "def to_tokens(text: str):\n",
    "    return tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "\n",
    "prompt = PROMPT_TEMPLATE.render(title=title, abstract=abstract)\n",
    "response = RESPONSE_TEMPLATE.render(paths=paths)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "    {\"role\": \"assistant\", \"content\": response},\n",
    "]\n",
    "full = MISTRAL_TEMPLATE.render(\n",
    "    messages=messages, bos_token=tokenizer.bos_token, eos_token=tokenizer.eos_token\n",
    ")\n",
    "print(full)\n",
    "\n",
    "parts = [\n",
    "    (f\"{tokenizer.bos_token}\", 0),\n",
    "    (f\"[INST] Title: {title}\\n{abstract} [/INST]\", 0),\n",
    "]\n",
    "for path in paths:\n",
    "    for i, item in enumerate(path):\n",
    "        parts.append((item, 1))\n",
    "        if i < len(path) - 1:\n",
    "            parts.append((\"->\", 0))\n",
    "    parts.append((\"\\n\", 2))\n",
    "parts.append((f\"{tokenizer.eos_token}\", 2))\n",
    "print(parts)\n",
    "\n",
    "tokens = []\n",
    "weights = []\n",
    "for part, w in parts:\n",
    "    part_tokens = to_tokens(part)\n",
    "    tokens += part_tokens\n",
    "    weights += [w] * len(part_tokens)\n",
    "print(tokenizer.decode(tokens))\n",
    "print(tokens)\n",
    "print(weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-ol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
