{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import dotenv\n",
    "import random\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers.tools import EncodingVisualizer\n",
    "\n",
    "from llm_ol.experiments.llm.templates import (\n",
    "    MISTRAL_TEMPLATE,\n",
    "    PROMPT_TEMPLATE,\n",
    "    RESPONSE_TEMPLATE,\n",
    ")\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"alpindale/Mistral-7B-v0.2-hf\", add_prefix_space=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out/experiments/llm/v2/train_dataset.jsonl\") as f:\n",
    "    items = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = random.choice(items)\n",
    "title, abstract, paths = item[\"title\"], item[\"abstract\"], item[\"paths\"]\n",
    "\n",
    "\n",
    "def to_tokens(text: str):\n",
    "    return tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "\n",
    "prompt = PROMPT_TEMPLATE.render(title=title, abstract=abstract)\n",
    "response = RESPONSE_TEMPLATE.render(paths=paths)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "    {\"role\": \"assistant\", \"content\": response},\n",
    "]\n",
    "full = MISTRAL_TEMPLATE.render(\n",
    "    messages=messages, bos_token=tokenizer.bos_token, eos_token=tokenizer.eos_token\n",
    ")\n",
    "print(full)\n",
    "full_tokens = to_tokens(full)\n",
    "\n",
    "inst_end = [733, 28748, 16289, 28793]\n",
    "arrow = 3193\n",
    "linebreak = 13\n",
    "\n",
    "\n",
    "def find_index(list_, sublist):\n",
    "    for i in range(len(list_) - len(sublist) + 1):\n",
    "        if list_[i : i + len(sublist)] == sublist:\n",
    "            return i\n",
    "    raise ValueError(f\"Sublist {sublist} not found in list\")\n",
    "\n",
    "\n",
    "resp_start_idx = find_index(full_tokens, inst_end) + len(inst_end)\n",
    "# resp_parts = [[[]]]\n",
    "# for token in full_tokens[resp_start_idx:]:\n",
    "#     if token == linebreak:\n",
    "#         resp_parts.append([[]])\n",
    "#     elif token == arrow:\n",
    "#         resp_parts[-1].append([])\n",
    "#     else:\n",
    "#         resp_parts[-1][-1].append(token)\n",
    "\n",
    "weights = [0] * resp_start_idx\n",
    "word = []\n",
    "for token in full_tokens[resp_start_idx:]:\n",
    "    if token == linebreak or token == arrow:\n",
    "        # print(repr(tokenizer.decode(word)))\n",
    "        weights += [1] * len(word) + [2]\n",
    "        word = []\n",
    "    elif token == tokenizer.eos_token_id:\n",
    "        weights += [4]\n",
    "    # elif token == arrow:\n",
    "    #     print(repr(tokenizer.decode(word)))\n",
    "    #     word = []\n",
    "    else:\n",
    "        word.append(token)\n",
    "\n",
    "\n",
    "tokens_per_line = 20\n",
    "\n",
    "for i in range(0, len(full_tokens), tokens_per_line):\n",
    "    tokens = [\n",
    "        tokenizer.convert_ids_to_tokens(ids)\n",
    "        for ids in full_tokens[i : i + tokens_per_line]\n",
    "    ]\n",
    "    weight = weights[i : i + tokens_per_line]\n",
    "    print(list(zip(tokens, weight)))\n",
    "\n",
    "# for token, weight in zip(full_tokens, weights):\n",
    "#     print(repr(tokenizer.decode(token)), weight)\n",
    "\n",
    "# for path in resp_parts:\n",
    "#     for words in path:\n",
    "#         print(repr(tokenizer.decode(words)))\n",
    "#     print()\n",
    "\n",
    "# parts = [\n",
    "#     (f\"{tokenizer.bos_token}\", 0),\n",
    "#     (f\"[INST] Title: {title}\\n{abstract} [/INST]\", 0),\n",
    "# ]\n",
    "# for path in paths:\n",
    "#     for i, item in enumerate(path):\n",
    "#         parts.append((item, 1))\n",
    "#         if i < len(path) - 1:\n",
    "#             parts.append((\"->\", 0))\n",
    "#     parts.append((\"\\n\", 2))\n",
    "# parts.append((f\"{tokenizer.eos_token}\", 2))\n",
    "# print(parts)\n",
    "\n",
    "# tokens = []\n",
    "# weights = []\n",
    "# for part, w in parts:\n",
    "#     part_tokens = to_tokens(part)\n",
    "#     tokens += part_tokens\n",
    "#     weights += [w] * len(part_tokens)\n",
    "# print(tokenizer.decode(tokens))\n",
    "# print(tokens)\n",
    "# print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(full, add_special_tokens=False)\n",
    "tokens_per_line = 20\n",
    "\n",
    "for i in range(0, len(tokens), tokens_per_line):\n",
    "    toks = tokens[i : i + tokens_per_line]\n",
    "    ids = tokenizer.convert_tokens_to_ids(toks)\n",
    "    print(list(zip(toks, ids)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.encode(\"->\", add_special_tokens=False)\n",
    "tokenizer.encode(\"\\n\", add_special_tokens=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-ol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
