{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from llm_ol.dataset import data_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_file = Path(\"out/data/wikipedia/v1/full/full_graph.json\")\n",
    "graph_file = Path(\"out/data/wikipedia/v2/full/graph_depth_3.json\")\n",
    "# graph_file = Path(\"out/data/wikipedia/v2/train_test_split/test_graph.json\")\n",
    "\n",
    "G = data_model.load_graph(graph_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = nx.number_of_nodes(G)\n",
    "num_edges = nx.number_of_edges(G)\n",
    "\n",
    "print(f\"Number of nodes: {num_nodes:,}\")\n",
    "print(f\"Number of edges: {num_edges:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degrees = [G.in_degree(n) for n in G.nodes]\n",
    "out_degrees = [G.out_degree(n) for n in G.nodes]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.hist(in_degrees, bins=21, log=True)\n",
    "ax1.set(xlabel=\"In-degree\", ylabel=\"Count\")\n",
    "ax2.hist(out_degrees, bins=100, log=True)\n",
    "ax2.set(xlabel=\"Out-degree\", ylabel=\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_sizes = [len(c) for c in nx.strongly_connected_components(G)]\n",
    "component_sizes = pd.DataFrame(component_sizes, columns=[\"size\"])\n",
    "component_sizes.groupby(\"size\").size().reset_index(name=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pages = []\n",
    "num_text = []\n",
    "for node, data in G.nodes(data=True):\n",
    "    num_pages.append(len(data[\"pages\"]))\n",
    "    num_text += [len(p[\"abstract\"]) for p in data[\"pages\"]]\n",
    "page_count = sum(num_pages)\n",
    "text_count = sum(num_text)\n",
    "\n",
    "print(f\"Number of pages: {page_count:,}\")\n",
    "print(f\"Avg. pages per node: {page_count/num_nodes:.2f}\")\n",
    "print(f\"Number of characters: {text_count:,} = ~{int(text_count/4):,} tokens\")\n",
    "print(f\"Avg. characters per page: {text_count/page_count:.2f}\")\n",
    "print(f\"Avg. characters per node: {text_count/num_nodes:.2f}\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.hist(num_pages, bins=50, log=True)\n",
    "ax1.set(xlabel=\"Number of pages\", ylabel=\"Number of nodes\")\n",
    "ax2.hist(num_text, bins=50, log=True)\n",
    "ax2.set(xlabel=\"Number of characters\", ylabel=\"Number of pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_ol.dataset.wikipedia.build_categories import ROOT_CATEGORY_ID\n",
    "\n",
    "lenghts = list(nx.single_source_shortest_path_length(G, ROOT_CATEGORY_ID).values())\n",
    "df = pd.DataFrame(lenghts, columns=[\"length\"])\n",
    "df.groupby(\"length\").size().reset_index(name=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = [data[\"title\"] for _, data in G.nodes(data=True)]\n",
    "\n",
    "print(len(category_names))\n",
    "print(len(set(category_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_words = {\n",
    "    \"wikipedia\",\n",
    "    \"wikiproject\",\n",
    "    \"list\",\n",
    "    \"lists\",\n",
    "    \"mediawiki\",\n",
    "    \"template\",\n",
    "    \"templates\",\n",
    "    \"user\",\n",
    "    \"users\",\n",
    "    \"portal\",\n",
    "    \"portal\",\n",
    "    \"category\",\n",
    "    \"categories\",\n",
    "    \"article\",\n",
    "    \"page\",\n",
    "}\n",
    "\n",
    "\n",
    "def is_special(name):\n",
    "    return any(\n",
    "        any(word == special_word for word in name.lower().split())\n",
    "        for special_word in special_words\n",
    "    )\n",
    "\n",
    "\n",
    "filtered_names = [name for name in category_names if is_special(name)]\n",
    "\n",
    "print(len(filtered_names))\n",
    "print(f\"{len(filtered_names)/len(category_names):.2%}\")\n",
    "print(filtered_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-ol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
