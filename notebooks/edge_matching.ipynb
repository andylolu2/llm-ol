{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%env OMP_NUM_THREADS=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import graph_tool\n",
    "\n",
    "import dotenv\n",
    "\n",
    "import networkx as nx\n",
    "import torch\n",
    "from absl import logging\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.nn import GCNConv, SGConv\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "from llm_ol.llm.embed import embed, load_embedding_model\n",
    "from llm_ol.dataset import data_model\n",
    "from llm_ol.utils import batch, textqdm\n",
    "from llm_ol.eval.graph_metrics import embed_graph\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "logging.set_verbosity(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "G_1 = data_model.load_graph(\"out/data/wikipedia/v2/train_test_split/test_graph.json\")\n",
    "G_2 = data_model.load_graph(\"out/experiments/prompting/v5/eval/graph.json\")\n",
    "\n",
    "G_1.number_of_nodes(), G_2.number_of_nodes(), G_1.number_of_edges(), G_2.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "embedder, tokenizer = load_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "\n",
    "for nodes in batch(textqdm(G.nodes), 100):\n",
    "    texts = [G.nodes[n][\"title\"] for n in nodes]\n",
    "    embeds = embed(texts, embedder, tokenizer)\n",
    "    for n, e in zip(nodes, embeds):\n",
    "        embeddings[n] = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# a1, b1 = \"Leaders of the world\", \"Presidents of the United States\"\n",
    "# a2, b2 = b1, a1\n",
    "# a2, b2 = \"World leaders\", \"US Presidents\"\n",
    "\n",
    "\n",
    "# def edge_sim_v1(u1, v1, u2, v2):\n",
    "#     ex1_emb = embed(u1, embedder, tokenizer) + orth @ embed(v1, embedder, tokenizer)\n",
    "#     ex2_emb = embed(u2, embedder, tokenizer) + orth @ embed(v2, embedder, tokenizer)\n",
    "#     sim = torch.nn.functional.cosine_similarity(ex1_emb, ex2_emb, dim=-1)\n",
    "#     return sim\n",
    "\n",
    "\n",
    "def edge_sim_v2(edges1, edges2):\n",
    "    u1_emb = torch.stack([embeddings[u1] for u1, _ in edges1])\n",
    "    v1_emb = torch.stack([embeddings[v1] for _, v1 in edges1])\n",
    "    u2_emb = torch.stack([embeddings[u2] for u2, _ in edges2])\n",
    "    v2_emb = torch.stack([embeddings[v2] for _, v2 in edges2])\n",
    "    u1_emb = u1_emb / u1_emb.norm(dim=-1, keepdim=True)\n",
    "    v1_emb = v1_emb / v1_emb.norm(dim=-1, keepdim=True)\n",
    "    u2_emb = u2_emb / u2_emb.norm(dim=-1, keepdim=True)\n",
    "    v2_emb = v2_emb / v2_emb.norm(dim=-1, keepdim=True)\n",
    "    sim_1 = u1_emb @ u2_emb.T\n",
    "    sim_2 = v1_emb @ v2_emb.T\n",
    "    return sim_1 * sim_2\n",
    "\n",
    "\n",
    "# print(f\"v1: {edge_sim_v1(a1, b1, a2, b2)}\")\n",
    "# print(f\"v2: {edge_sim_v2(a1, b1, a2, b2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "edges = list(G.edges)\n",
    "\n",
    "sims = []\n",
    "for edge_batch_1 in batch(textqdm(edges), 128):\n",
    "    sim = []\n",
    "    for edge_batch_2 in batch(textqdm(edges), 128):\n",
    "        sim.append(edge_sim_v2(edge_batch_1, edge_batch_2))\n",
    "    sims.append(torch.cat(sim, dim=-1))\n",
    "sims = torch.cat(sims, dim=0)\n",
    "sims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "idx = torch.randint(0, len(edges), (1,)).item()\n",
    "top_k = sims[idx].topk(6).indices\n",
    "\n",
    "u, v = edges[idx]\n",
    "print(f\"{G.nodes[u]['title']} -> {G.nodes[v]['title']}\")\n",
    "for i in top_k[1:]:\n",
    "    u, v = edges[i]\n",
    "    print(f\"\\t{G.nodes[u]['title']} -> {G.nodes[v]['title']} ({sims[idx, i]:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def graph_similarity(\n",
    "    G: nx.DiGraph,\n",
    "    n_iters: int = 3,\n",
    "    embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "):\n",
    "    def nx_to_vec(G: nx.Graph, n_iters) -> torch.Tensor:\n",
    "        \"\"\"Compute a graph embedding of shape (n_nodes embed_dim).\n",
    "\n",
    "        Uses a GCN with identity weights to compute the embedding.\n",
    "        \"\"\"\n",
    "\n",
    "        # Delete all node and edge attributes except for the embedding\n",
    "        # Otherwise PyG might complain \"Not all nodes/edges contain the same attributes\"\n",
    "        G = G.copy()\n",
    "        for _, _, d in G.edges(data=True):\n",
    "            d.clear()\n",
    "        for _, d in G.nodes(data=True):\n",
    "            for k in list(d.keys()):\n",
    "                if k != \"embed\":\n",
    "                    del d[k]\n",
    "        pyg_G = from_networkx(G, group_node_attrs=[\"embed\"])\n",
    "\n",
    "        embed_dim = pyg_G.x.shape[1]\n",
    "        conv = SGConv(embed_dim, embed_dim, K=n_iters, bias=False)\n",
    "        conv.lin.weight.data = torch.eye(embed_dim, device=conv.lin.weight.device)\n",
    "\n",
    "        pyg_batch = Batch.from_data_list([pyg_G])\n",
    "        x, edge_index = pyg_batch.x, pyg_batch.edge_index  # type: ignore\n",
    "        # x, edge_index = x.to(device), edge_index.to(device)\n",
    "        x = conv(x, edge_index)\n",
    "\n",
    "        # for _ in range(n_iters):\n",
    "        #     x = conv(x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "    if \"embed\" not in G.nodes[next(iter(G.nodes))]:\n",
    "        G = embed_graph(G, embedding_model=embedding_model)\n",
    "\n",
    "    return nx_to_vec(G, n_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "G1_embed = graph_similarity(G_1)\n",
    "G2_embed = graph_similarity(G_2)\n",
    "G1_embed.shape, G2_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "G1_embed = G1_embed / G1_embed.norm(dim=-1, keepdim=True)\n",
    "G2_embed = G2_embed / G2_embed.norm(dim=-1, keepdim=True)\n",
    "sim = G1_embed @ G2_embed.T\n",
    "sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "row_ind, col_ind = linear_sum_assignment(sim.cpu().numpy(), maximize=True)\n",
    "row_ind.shape, col_ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "cost = sim[row_ind, col_ind].sum().item() / len(row_ind)\n",
    "cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-ol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
