{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch import nn\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.data import Batch, Data\n",
    "\n",
    "from llm_ol.dataset import wikipedia\n",
    "from llm_ol.utils.data import batch\n",
    "\n",
    "torch.set_num_threads(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = wikipedia.load_dataset(\n",
    "    Path(\"out/data/wikipedia/v1/full/full_graph.json\"), max_depth=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = (\n",
    "        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    )\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "        input_mask_expanded.sum(1), min=1e-9\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nodes in batch(tqdm(G.nodes), batch_size=64):\n",
    "    titles = [G.nodes[n][\"title\"] for n in nodes]\n",
    "    inputs = tokenizer(titles, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embed = mean_pooling(outputs, inputs[\"attention_mask\"])\n",
    "    for n, e in zip(nodes, embed):\n",
    "        G.nodes[n][\"embed\"] = e.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph augmentations\n",
    "\n",
    "\n",
    "def remove_edges(G: nx.Graph, p: float):\n",
    "    G = G.copy()\n",
    "    edges = list(G.edges)\n",
    "    n_edits = int(p * len(edges))\n",
    "    chosen = np.random.choice(len(edges), n_edits, replace=False)\n",
    "    for i in chosen:\n",
    "        u, v = edges[i]\n",
    "        G.remove_edge(u, v)\n",
    "    return G, n_edits\n",
    "\n",
    "\n",
    "def add_edges(G: nx.Graph, p: float):\n",
    "    G = G.copy()\n",
    "    all_edges = [\n",
    "        (u, v) for u in G.nodes for v in G.nodes if u != v and not G.has_edge(u, v)\n",
    "    ]\n",
    "    n_edits = int(p * len(all_edges))\n",
    "    chosen = np.random.choice(len(all_edges), n_edits, replace=False)\n",
    "    for i in chosen:\n",
    "        u, v = all_edges[i]\n",
    "        G.add_edge(u, v)\n",
    "    return G, n_edits\n",
    "\n",
    "\n",
    "def remove_nodes(G: nx.Graph, p: float):\n",
    "    G = G.copy()\n",
    "    nodes = list(G.nodes)\n",
    "    n_edits = int(p * len(nodes))\n",
    "    chosen = np.random.choice(len(nodes), n_edits, replace=False)\n",
    "    for i in chosen:\n",
    "        G.remove_node(nodes[i])\n",
    "    return G, n_edits\n",
    "\n",
    "\n",
    "def remove_subgraphs(G: nx.Graph, n: int):\n",
    "    G = G.copy()\n",
    "    for _ in range(n):\n",
    "        nodes = list(G.nodes)\n",
    "        node = np.random.choice(nodes)\n",
    "        subgraph = nx.ego_graph(G, node, radius=1, undirected=True)\n",
    "        G.remove_nodes_from(subgraph)\n",
    "    return G, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph2vec(pyg_G: Data, n_iters: int = 1) -> torch.Tensor:\n",
    "    input_dim = pyg_G.x.size(1)\n",
    "    conv = GCNConv(input_dim, input_dim, bias=False)\n",
    "    conv.lin.weight.data = torch.eye(input_dim)\n",
    "\n",
    "    pyg_batch = Batch.from_data_list([pyg_G])\n",
    "    x, edge_index = pyg_batch.x, pyg_batch.edge_index\n",
    "\n",
    "    for _ in range(n_iters):\n",
    "        with torch.no_grad():\n",
    "            x = conv(x, edge_index)\n",
    "            x = torch.tanh(x)\n",
    "\n",
    "    # [x] = global_mean_pool(pyg_batch.x, pyg_batch.batch)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_dist(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    # similarity matrix\n",
    "    a = a.unsqueeze(1)\n",
    "    b = b.unsqueeze(0)\n",
    "    sim = torch.nn.functional.cosine_similarity(a, b, dim=-1)\n",
    "    # sim = a @ b.T\n",
    "\n",
    "    return (sim.max(0).values.mean() + sim.max(1).values.mean()) / 2\n",
    "    # return sim.mean()\n",
    "\n",
    "\n",
    "def nx_to_vec(G: nx.Graph):\n",
    "    # Delete all edge attributes\n",
    "    for _, _, d in G.edges(data=True):\n",
    "        d.clear()\n",
    "\n",
    "    # Delete all node attributes except for the embedding\n",
    "    for _, d in G.nodes(data=True):\n",
    "        for k in list(d.keys()):\n",
    "            if k != \"embed\":\n",
    "                del d[k]\n",
    "\n",
    "    return graph2vec(from_networkx(G, group_node_attrs=[\"embed\"]), n_iters=10)\n",
    "\n",
    "\n",
    "vec_orig = nx_to_vec(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = {\n",
    "    \"Remove random edges\": (remove_edges, [0, 0.25, 0.5, 0.75, 1]),\n",
    "    \"Add random edges\": (add_edges, [0, 0.001, 0.002, 0.003, 0.004, 0.005]),\n",
    "    \"Remove random nodes\": (remove_nodes, [0, 0.2, 0.4, 0.6, 0.8]),\n",
    "    \"Remove random 1-subgraphs\": (remove_subgraphs, [0, 10, 20, 30, 40, 50]),\n",
    "}\n",
    "\n",
    "data = []\n",
    "for method, (f, ps) in methods.items():\n",
    "    for p in ps:\n",
    "        for _ in range(5):\n",
    "            G_aug, n_edits = f(G, p)\n",
    "            vec_aug = nx_to_vec(G_aug)\n",
    "            dist = embedding_dist(vec_orig, vec_aug)\n",
    "            data.append({\"method\": method, \"dist\": dist.item(), \"n_edits\": n_edits})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=len(methods), figsize=(20, 4), sharey=True)\n",
    "for ax, method in zip(axs, methods):\n",
    "    sns.lineplot(x=\"n_edits\", y=\"dist\", data=df[df.method == method], ax=ax)\n",
    "    ax.set(\n",
    "        title=method,\n",
    "        xlabel=\"No. of edits\",\n",
    "        ylabel=\"Metric\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_hearst = wikipedia.load_dataset(\"out/experiments/hearst/v1/graph.json\")\n",
    "\n",
    "for nodes in batch(tqdm(G_hearst.nodes), batch_size=64):\n",
    "    titles = [G_hearst.nodes[n][\"title\"] for n in nodes]\n",
    "    inputs = tokenizer(titles, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embed = mean_pooling(outputs, inputs[\"attention_mask\"])\n",
    "    for n, e in zip(nodes, embed):\n",
    "        G_hearst.nodes[n][\"embed\"] = e.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = {\n",
    "    \"Remove random edges\": (remove_edges, [0, 0.25, 0.5, 0.75, 1]),\n",
    "    \"Add random edges\": (add_edges, [0, 2e-4, 4e-4, 6e-4, 8e-4, 1e-3]),\n",
    "    \"Remove random nodes\": (remove_nodes, [0, 0.2, 0.4, 0.6, 0.8]),\n",
    "    \"Remove random 1-subgraphs\": (remove_subgraphs, [0, 30, 60, 90, 120, 150]),\n",
    "}\n",
    "\n",
    "data_hearst = []\n",
    "for method, (f, ps) in methods.items():\n",
    "    for p in ps:\n",
    "        for _ in range(5):\n",
    "            G_aug, n_edits = f(G_hearst, p)\n",
    "            vec_aug = nx_to_vec(G_aug)\n",
    "            dist = embedding_dist(vec_orig, vec_aug)\n",
    "            data_hearst.append(\n",
    "                {\"method\": method, \"dist\": dist.item(), \"n_edits\": n_edits}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_hearst)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=len(methods), figsize=(20, 4), sharey=True)\n",
    "for ax, method in zip(axs, methods):\n",
    "    sns.lineplot(x=\"n_edits\", y=\"dist\", data=df[df.method == method], ax=ax)\n",
    "    ax.set(\n",
    "        title=method,\n",
    "        xlabel=\"No. of edits\",\n",
    "        ylabel=\"Metric\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
