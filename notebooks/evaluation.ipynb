{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch import nn\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Batch, Data\n",
    "\n",
    "from llm_ol.dataset import wikipedia\n",
    "from llm_ol.utils.data import batch\n",
    "\n",
    "torch.set_num_threads(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = wikipedia.load_dataset(Path(\"out/data/wikipedia/v1/full/graph_depth_3.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = (\n",
    "        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    )\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "        input_mask_expanded.sum(1), min=1e-9\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nodes in batch(tqdm(G.nodes), batch_size=64):\n",
    "    titles = [G.nodes[n][\"title\"] for n in nodes]\n",
    "    inputs = tokenizer(titles, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embed = mean_pooling(outputs, inputs[\"attention_mask\"])\n",
    "    for n, e in zip(nodes, embed):\n",
    "        G.nodes[n][\"embed\"] = e.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph augmentations\n",
    "\n",
    "\n",
    "def remove_edges(G: nx.Graph, p: float):\n",
    "    G = G.copy()\n",
    "    edges = list(G.edges)\n",
    "    n_edits = int(p * len(edges))\n",
    "    chosen = np.random.choice(len(edges), n_edits, replace=False)\n",
    "    for i in chosen:\n",
    "        u, v = edges[i]\n",
    "        G.remove_edge(u, v)\n",
    "    return G, n_edits\n",
    "\n",
    "\n",
    "def add_edges(G: nx.Graph, p: float):\n",
    "    G = G.copy()\n",
    "    all_edges = [\n",
    "        (u, v) for u in G.nodes for v in G.nodes if u != v and not G.has_edge(u, v)\n",
    "    ]\n",
    "    n_edits = int(p * len(all_edges))\n",
    "    chosen = np.random.choice(len(all_edges), n_edits, replace=False)\n",
    "    for i in chosen:\n",
    "        u, v = all_edges[i]\n",
    "        G.add_edge(u, v)\n",
    "    return G, n_edits\n",
    "\n",
    "\n",
    "def remove_nodes(G: nx.Graph, p: float):\n",
    "    G = G.copy()\n",
    "    nodes = list(G.nodes)\n",
    "    n_edits = int(p * len(nodes))\n",
    "    chosen = np.random.choice(len(nodes), n_edits, replace=False)\n",
    "    for i in chosen:\n",
    "        G.remove_node(nodes[i])\n",
    "    return G, n_edits\n",
    "\n",
    "\n",
    "def remove_subgraphs(G: nx.Graph, n: int):\n",
    "    G = G.copy()\n",
    "    for _ in range(n):\n",
    "        nodes = list(G.nodes)\n",
    "        node = np.random.choice(nodes)\n",
    "        subgraph = nx.ego_graph(G, node, radius=1, undirected=True)\n",
    "        G.remove_nodes_from(subgraph)\n",
    "    return G, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def graph2vec(pyg_G: Data, n_iters: int = 1) -> torch.Tensor:\n",
    "    input_dim = pyg_G.x.size(1)\n",
    "    conv = GCNConv(input_dim, input_dim, bias=False)\n",
    "    conv.lin.weight.data = torch.eye(input_dim)\n",
    "\n",
    "    pyg_batch = Batch.from_data_list([pyg_G])\n",
    "    x, edge_index = pyg_batch.x, pyg_batch.edge_index\n",
    "\n",
    "    for _ in range(n_iters):\n",
    "        x = conv(x, edge_index)\n",
    "        # x = torch.tanh(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_dist(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    a = a / a.norm(dim=-1, keepdim=True)\n",
    "    b = b / b.norm(dim=-1, keepdim=True)\n",
    "    sim = a @ b.T\n",
    "\n",
    "    return (sim.max(0).values.mean() + sim.max(1).values.mean()) / 2\n",
    "\n",
    "\n",
    "def nx_to_vec(G: nx.Graph, n_iters: int = 5):\n",
    "    G = G.copy()\n",
    "    # Delete all edge attributes\n",
    "    for _, _, d in G.edges(data=True):\n",
    "        d.clear()\n",
    "\n",
    "    # Delete all node attributes except for the embedding\n",
    "    for _, d in G.nodes(data=True):\n",
    "        for k in list(d.keys()):\n",
    "            if k != \"embed\":\n",
    "                del d[k]\n",
    "\n",
    "    return graph2vec(from_networkx(G, group_node_attrs=[\"embed\"]), n_iters)\n",
    "\n",
    "\n",
    "vec_orig = nx_to_vec(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = {\n",
    "    \"Remove random edges\": (remove_edges, [0, 0.25, 0.5, 0.75, 1]),\n",
    "    \"Add random edges\": (add_edges, [0, 3e-4, 6e-4, 9e-4, 12e-4]),\n",
    "    \"Remove random nodes\": (remove_nodes, [0, 0.2, 0.4, 0.6, 0.8]),\n",
    "    # \"Remove random 1-subgraphs\": (remove_subgraphs, [0, 10, 20, 30, 40, 50]),\n",
    "}\n",
    "\n",
    "data = []\n",
    "for method, (f, ps) in methods.items():\n",
    "    for p in ps:\n",
    "        for _ in range(5):\n",
    "            G_aug, n_edits = f(G, p)\n",
    "            vec_aug = nx_to_vec(G_aug)\n",
    "            dist = embedding_dist(vec_orig, vec_aug)\n",
    "            data.append({\"method\": method, \"dist\": dist.item(), \"n_edits\": n_edits})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=len(methods), figsize=(6 * len(methods), 4), sharey=True)\n",
    "for ax, method in zip(axs, methods):\n",
    "    sns.lineplot(x=\"n_edits\", y=\"dist\", data=df[df.method == method], ax=ax)\n",
    "    ax.set(\n",
    "        title=method,\n",
    "        xlabel=\"No. of edits\",\n",
    "        ylabel=\"Metric\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_hearst = wikipedia.load_dataset(\"out/experiments/hearst/v1/graph.json\")\n",
    "\n",
    "for nodes in batch(tqdm(G_hearst.nodes), batch_size=64):\n",
    "    titles = [G_hearst.nodes[n][\"title\"] for n in nodes]\n",
    "    inputs = tokenizer(titles, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embed = mean_pooling(outputs, inputs[\"attention_mask\"])\n",
    "    for n, e in zip(nodes, embed):\n",
    "        G_hearst.nodes[n][\"embed\"] = e.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = {\n",
    "    \"Remove random edges\": (remove_edges, [0, 0.25, 0.5, 0.75, 1]),\n",
    "    \"Add random edges\": (add_edges, [0, 2e-6, 4e-6, 6e-6, 8e-6, 1e-5]),\n",
    "    \"Remove random nodes\": (remove_nodes, [0, 0.2, 0.4, 0.6, 0.8]),\n",
    "    # \"Remove random 1-subgraphs\": (remove_subgraphs, [0, 30, 60, 90, 120, 150]),\n",
    "}\n",
    "\n",
    "data_hearst = []\n",
    "for method, (f, ps) in methods.items():\n",
    "    for p in ps:\n",
    "        for _ in range(5):\n",
    "            G_aug, n_edits = f(G_hearst, p)\n",
    "            vec_aug = nx_to_vec(G_aug)\n",
    "            dist = embedding_dist(vec_orig, vec_aug)\n",
    "            data_hearst.append(\n",
    "                {\"method\": method, \"dist\": dist.item(), \"n_edits\": n_edits}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_hearst)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=len(methods), figsize=(6 * len(methods), 4), sharey=True)\n",
    "for ax, method in zip(axs, methods):\n",
    "    sns.lineplot(x=\"n_edits\", y=\"dist\", data=df[df.method == method], ax=ax)\n",
    "    ax.set(\n",
    "        title=method,\n",
    "        xlabel=\"No. of edits\",\n",
    "        ylabel=\"Metric\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_prompting = wikipedia.load_dataset(\"out/experiments/prompting/dev-h/graph.json\")\n",
    "\n",
    "for nodes in batch(tqdm(G_prompting.nodes), batch_size=64):\n",
    "    titles = [G_prompting.nodes[n][\"title\"] for n in nodes]\n",
    "    inputs = tokenizer(titles, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embed = mean_pooling(outputs, inputs[\"attention_mask\"])\n",
    "    for n, e in zip(nodes, embed):\n",
    "        G_prompting.nodes[n][\"embed\"] = e.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = {\n",
    "    \"Remove random edges\": (remove_edges, [0, 0.25, 0.5, 0.75, 1]),\n",
    "    # \"Add random edges\": (add_edges, [0, 2e-4, 4e-4, 6e-4, 8e-4, 1e-3]),\n",
    "    \"Remove random nodes\": (remove_nodes, [0, 0.2, 0.4, 0.6, 0.8]),\n",
    "    # \"Remove random 1-subgraphs\": (remove_subgraphs, [0, 30, 60, 90, 120, 150]),\n",
    "}\n",
    "\n",
    "data_prompting = []\n",
    "for method, (f, ps) in methods.items():\n",
    "    for p in ps:\n",
    "        for _ in range(5):\n",
    "            G_aug, n_edits = f(G_prompting, p)\n",
    "            vec_aug = nx_to_vec(G_aug)\n",
    "            dist = embedding_dist(vec_orig, vec_aug)\n",
    "            data_prompting.append(\n",
    "                {\"method\": method, \"dist\": dist.item(), \"n_edits\": n_edits}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_prompting)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=len(methods), figsize=(6 * len(methods), 4), sharey=True)\n",
    "for ax, method in zip(axs, methods):\n",
    "    sns.lineplot(x=\"n_edits\", y=\"dist\", data=df[df.method == method], ax=ax)\n",
    "    ax.set(\n",
    "        title=method,\n",
    "        xlabel=\"No. of edits\",\n",
    "        ylabel=\"Metric\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_compare = []\n",
    "\n",
    "for n_iter in range(5):\n",
    "    vec_orig = nx_to_vec(G, n_iter)\n",
    "    vec_prompting = nx_to_vec(G_prompting, n_iter)\n",
    "    vec_hearst = nx_to_vec(G_hearst, n_iter)\n",
    "    data_compare.append(\n",
    "        {\n",
    "            \"n_iter\": n_iter,\n",
    "            \"prompting\": embedding_dist(vec_orig, vec_prompting).item(),\n",
    "            \"hearst\": embedding_dist(vec_orig, vec_hearst).item(),\n",
    "            \"variant\": \"parent -> child\",\n",
    "        }\n",
    "    )\n",
    "    vec_orig_rev = nx_to_vec(G.reverse(), n_iter)\n",
    "    vec_prompting_rev = nx_to_vec(G_prompting.reverse(), n_iter)\n",
    "    vec_hearst_rev = nx_to_vec(G_hearst.reverse(), n_iter)\n",
    "    data_compare.append(\n",
    "        {\n",
    "            \"n_iter\": n_iter,\n",
    "            \"prompting\": embedding_dist(vec_orig_rev, vec_prompting_rev).item(),\n",
    "            \"hearst\": embedding_dist(vec_orig_rev, vec_hearst_rev).item(),\n",
    "            \"variant\": \"child -> parent\",\n",
    "        }\n",
    "    )\n",
    "    vec_orig_uni = nx_to_vec(G.to_undirected(), n_iter)\n",
    "    vec_prompting_uni = nx_to_vec(G_prompting.to_undirected(), n_iter)\n",
    "    vec_hearst_uni = nx_to_vec(G_hearst.to_undirected(), n_iter)\n",
    "    data_compare.append(\n",
    "        {\n",
    "            \"n_iter\": n_iter,\n",
    "            \"prompting\": embedding_dist(vec_orig_uni, vec_prompting_uni).item(),\n",
    "            \"hearst\": embedding_dist(vec_orig_uni, vec_hearst_uni).item(),\n",
    "            \"variant\": \"undirected\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_compare)\n",
    "df = df.melt(id_vars=[\"n_iter\", \"variant\"], value_vars=[\"prompting\", \"hearst\"])\n",
    "df = df.rename(columns={\"variant\": \"Averaging direction\", \"variable\": \"Method\"})\n",
    "df[\"Averaging direction\"] = df[\"Averaging direction\"].str.replace(\"->\", \"$\\\\to$\")\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "ax = sns.lineplot(\n",
    "    x=\"n_iter\",\n",
    "    y=\"value\",\n",
    "    hue=\"Method\",\n",
    "    style=\"Averaging direction\",\n",
    "    data=df,\n",
    "    marker=\"o\",\n",
    ")\n",
    "ax.set(\n",
    "    xlabel=\"No. of iterations\",\n",
    "    ylabel=\"Distance to ground truth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.density(G), nx.density(G_hearst), nx.density(G_prompting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
