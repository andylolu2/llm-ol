{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "from llm_ol.dataset import wikipedia\n",
    "\n",
    "torch.set_num_threads(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path(\"out/data/wikipedia/v1/full/full_graph.json\")\n",
    "\n",
    "G = wikipedia.load_dataset(file_path, max_depth=1)\n",
    "seen = set()\n",
    "titles = []\n",
    "abstracts = []\n",
    "for _, data in G.nodes(data=True):\n",
    "    for page in data[\"pages\"]:\n",
    "        if page[\"title\"] in seen:\n",
    "            continue\n",
    "        seen.add(page[\"title\"])\n",
    "        titles.append(page[\"title\"])\n",
    "        abstracts.append(page[\"abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in G.nodes:\n",
    "    print(node, G.nodes[node][\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "\n",
    "batch_size = 128\n",
    "for i in range(0, len(abstracts), batch_size):\n",
    "    print(f\"Processing item {i} to {i + batch_size}\")\n",
    "    batch = abstracts[i : i + batch_size]\n",
    "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embed = outputs.last_hidden_state[:, 0, :]\n",
    "    embeddings.append(embed)\n",
    "\n",
    "embeddings = torch.cat(embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = embeddings.numpy()\n",
    "\n",
    "# distances = cosine_distances(X)\n",
    "clusterer = AgglomerativeClustering(n_clusters=42, compute_distances=True)\n",
    "clusterer.fit(X)\n",
    "\n",
    "# hierarchical graph\n",
    "H = nx.DiGraph()\n",
    "for title in titles:\n",
    "    H.add_node(title)\n",
    "\n",
    "for i, merge in enumerate(clusterer.children_):\n",
    "    H.add_node(i)\n",
    "    for child in merge:\n",
    "        if child < len(titles):\n",
    "            H.add_edge(i, titles[child])\n",
    "        else:\n",
    "            H.add_edge(i, child - len(titles))\n",
    "\n",
    "# print cluster sizes\n",
    "# cluster_sizes = {}\n",
    "# for i in range(clusterer.n_clusters):\n",
    "#     cluster_sizes[i] = 0\n",
    "#     for j in range(len(clusterer.labels_)):\n",
    "#         if clusterer.labels_[j] == i:\n",
    "#             cluster_sizes[i] += 1\n",
    "#     print(f\"Cluster {i}: {cluster_sizes[i]}\")\n",
    "\n",
    "\n",
    "# # Visualize clusters\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# palette = sns.color_palette(\"bright\", len(set(clusterer.labels_)))\n",
    "# colors = [palette[x] if x >= 0 else (0.5, 0.5, 0.5) for x in clusterer.labels_]\n",
    "\n",
    "# # plot cluster size distribution\n",
    "# sns.histplot(clusterer.labels_, ax=ax)\n",
    "\n",
    "# # Print titles in each cluster\n",
    "# for i in range(clusterer.n_clusters):\n",
    "#     print(f\"Cluster {i}\")\n",
    "#     for j in range(len(clusterer.labels_)):\n",
    "#         if clusterer.labels_[j] == i:\n",
    "#             print(f\"\\t{titles[j]}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot dendrogram\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "\n",
    "def get_all_children(model, node):\n",
    "    if node < len(titles):\n",
    "        return [node]\n",
    "    else:\n",
    "        left = model.children_[node - len(titles)][0]\n",
    "        right = model.children_[node - len(titles)][1]\n",
    "        return get_all_children(model, left) + get_all_children(model, right)\n",
    "\n",
    "\n",
    "def plot_dendrogram(X, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    # counts = np.zeros(model.children_.shape[0])\n",
    "    # n_samples = len(model.labels_)\n",
    "    # for i, merge in enumerate(model.children_):\n",
    "    #     current_count = 0\n",
    "    #     for child_idx in merge:\n",
    "    #         if child_idx < n_samples:\n",
    "    #             current_count += 1  # leaf node\n",
    "    #         else:\n",
    "    #             current_count += counts[child_idx - n_samples]\n",
    "    #     counts[i] = current_count\n",
    "\n",
    "    # linkage_matrix = np.column_stack(\n",
    "    #     [model.children_, model.distances_, counts]\n",
    "    # ).astype(float)\n",
    "\n",
    "    linkage_matrix = linkage(X, method=\"ward\")\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    return dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(14, 5), ncols=2)\n",
    "result = plot_dendrogram(\n",
    "    X, truncate_mode=\"lastp\", p=100, ax=ax1, get_leaves=True, distance_sort=True\n",
    ")\n",
    "ax1.set(title=\"Hierarchical Clustering Dendrogram\")\n",
    "\n",
    "sample_to_cluster = {}\n",
    "for i, leave in enumerate(result[\"leaves\"]):\n",
    "    for child in get_all_children(clusterer, leave):\n",
    "        sample_to_cluster[titles[child]] = i\n",
    "\n",
    "import random\n",
    "\n",
    "original_node = random.choice(list(G.nodes))\n",
    "original_name = G.nodes[original_node][\"title\"]\n",
    "print(original_name)\n",
    "\n",
    "reachable_pages = []\n",
    "for node in nx.descendants(G, original_node) | {original_node}:\n",
    "    for page in G.nodes[node][\"pages\"]:\n",
    "        reachable_pages.append(page[\"title\"])\n",
    "print(reachable_pages)\n",
    "\n",
    "ax2.hist(\n",
    "    [sample_to_cluster[x] for x in reachable_pages], bins=40, color=\"blue\", alpha=0.5\n",
    ")\n",
    "ax2.set(\n",
    "    xlim=(0, len(result[\"leaves\"])),\n",
    "    title=f\"Clustering of pages reachable from {original_name}\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-ol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
